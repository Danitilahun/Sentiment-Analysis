{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas seaborn matplotlib plotly scikit-learn nltk wordcloud pyspellchecker beautifulsoup4 prettytable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'C:\\Users\\tiled\\OneDrive\\Desktop\\DataSet\\IMDB-Dataset.csv',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    *Disclaimer: I only watched this movie as a co...\n",
       "1    I am writing this in hopes that this gets put ...\n",
       "2    Really, I could write a scathing review of thi...\n",
       "3    If you saw the other previous spoof movies by ...\n",
       "4    This movie I saw a day early for free and I st...\n",
       "5    Honestly, what is wrong with you, Hollywood? N...\n",
       "6    I was given a free ticket to this film; so I c...\n",
       "7    OK, so \"Disastrous\" isn't an imaginative barb ...\n",
       "8    Jason Friedberg and Aaron Seltzer, the way eve...\n",
       "9    Honestly the worst movie ever made. Theatre fu...\n",
       "Name: Reviews, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)[\"Reviews\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing Stopwords for Enhanced Text Preprocessing\n",
    "\n",
    "*Enhancing the default stopword list by adding domain-specific words and removing negations to improve text analysis accuracy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tiled\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "new_stopwords = [\n",
    "\"would\", \"shall\", \"could\", \"might\", \"movie\", \"movies\", \"film\", \"films\",\n",
    "\"cinema\", \"director\", \"directors\", \"actor\", \"actors\", \"actress\", \"actresses\",\n",
    "\"cast\", \"screenplay\", \"plot\", \"story\", \"character\", \"characters\", \"scene\",\n",
    "\"scenes\", \"soundtrack\", \"soundtracks\",\"sequel\", \"prequel\", \"adaptation\", \"trailer\", \"genre\", \"genres\", \"release\",\n",
    "\"theaters\", \"theatre\", \"performance\", \"performances\", \"acting\", \"blockbuster\",\n",
    "\"indie\", \"oscars\", \"oscar\", \"award-winning\", \"box\", \"office\", \"premiere\",\n",
    "\"rating\", \"ratings\", \"reviews\", \"animation\", \"cinematography\", \"editing\", \"script\", \"narrative\",\n",
    "\"direction\", \"score\", \"soundtrack\"\n",
    "]\n",
    "\n",
    "stop_words.extend(new_stopwords)\n",
    "stop_words.remove(\"not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233\n"
     ]
    }
   ],
   "source": [
    "stop_words=set(stop_words)\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Text Preprocessing Pipeline\n",
    "\n",
    "*Comprehensive functions for cleaning and preparing text data, including removing special characters, URLs, stopwords, and expanding contractions to enhance the quality of the dataset.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction of Typos\n",
    "\n",
    "*Written text often contains errors, such as “Fen” instead of “Fan.” To rectify these errors, a dictionary is employed to map words to their correct forms based on similarity.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "def correct_typos(content):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - content (str): The input text to be corrected.\n",
    "\n",
    "    Returns:\n",
    "    - str: The text with corrected typos.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        words = content.split()\n",
    "        corrected_words = []\n",
    "        misspelled = spell.unknown(words)\n",
    "\n",
    "        for word in words:\n",
    "            if word in misspelled:\n",
    "                corrected_word = spell.correction(word)\n",
    "                corrected_words.append(corrected_word)\n",
    "            else:\n",
    "                corrected_words.append(word)\n",
    "        \n",
    "        corrected_content = ' '.join(corrected_words)\n",
    "        return corrected_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error in correcting typos: {e}\")\n",
    "        return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping and Replacement\n",
    "\n",
    "*This involves mapping words to standardized language equivalents. For instance, words like “b4” and “ttyl,” commonly understood by humans as “before” and “talk to you later,” pose challenges for machines. Normalization entails mapping such words to their standardized counterparts.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def load_mapping_dictionary():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    - dict: A dictionary mapping non-standard words to standardized words.\n",
    "    \"\"\"\n",
    "    mapping_dict = {\n",
    "        \"b4\": \"before\",\n",
    "        \"ttyl\": \"talk to you later\",\n",
    "        \"u\": \"you\",\n",
    "        \"r\": \"are\",\n",
    "        \"lol\": \"laughing out loud\",\n",
    "        \"idk\": \"i do not know\",\n",
    "        \"btw\": \"by the way\",\n",
    "        \"omg\": \"oh my god\",\n",
    "        \"imo\": \"in my opinion\",\n",
    "        \"np\": \"no problem\",\n",
    "        \"gr8\": \"great\",\n",
    "        \"l8r\": \"later\",\n",
    "        \"gtg\": \"got to go\",\n",
    "        \"thx\": \"thanks\",\n",
    "        \"pls\": \"please\",\n",
    "        \"plz\": \"please\",\n",
    "        \"bc\": \"because\",\n",
    "        \"cuz\": \"because\",\n",
    "        \"y'all\": \"you all\",\n",
    "        \"luv\": \"love\",\n",
    "        \"wanna\": \"want to\",\n",
    "        \"gonna\": \"going to\",\n",
    "        \"hafta\": \"have to\",\n",
    "        \"kinda\": \"kind of\",\n",
    "        \"sorta\": \"sort of\",\n",
    "        \"gimme\": \"give me\",\n",
    "        \"lemme\": \"let me\",\n",
    "        \"whatcha\": \"what are you\",\n",
    "        \"whaddaya\": \"what do you\",\n",
    "    }\n",
    "    return mapping_dict\n",
    "\n",
    "def normalize_text(content, mapping_dict):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - content (str): The input text to be normalized.\n",
    "    - mapping_dict (dict): A dictionary mapping non-standard words to standardized words.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The normalized text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        words = content.split()\n",
    "        normalized_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            clean_word = re.sub(r'[^\\w\\s]', '', word.lower())\n",
    "            if clean_word in mapping_dict:\n",
    "                replacement = mapping_dict[clean_word]\n",
    "                normalized_words.append(replacement)\n",
    "            else:\n",
    "                normalized_words.append(word)\n",
    "        \n",
    "        normalized_content = ' '.join(normalized_words)\n",
    "        return normalized_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error in normalizing text: {e}\")\n",
    "        return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding Contractions\n",
    "\n",
    "*Convert contractions like “don’t” to “do not” or “I’ll” to “I will.”*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "        r\"won't\": \"will not\",\n",
    "        r\"can't\": \"cannot\",\n",
    "        r\"i'm\": \"i am\",\n",
    "        r\"ain't\": \"is not\",\n",
    "        r\"let's\": \"let us\",\n",
    "        r\"ma'am\": \"madam\",\n",
    "        r\"shan't\": \"shall not\",\n",
    "        r\"n't\": \" not\",\n",
    "        r\"'re\": \" are\",\n",
    "        r\"'s\": \" is\",\n",
    "        r\"'d\": \" would\",\n",
    "        r\"'ll\": \" will\",\n",
    "        r\"'ve\": \" have\",\n",
    "        r\"'m\": \" am\",\n",
    "        r\"he's\": \"he is\",\n",
    "        r\"she's\": \"she is\",\n",
    "        r\"it's\": \"it is\",\n",
    "        r\"that's\": \"that is\",\n",
    "        r\"there's\": \"there is\",\n",
    "        r\"who's\": \"who is\",\n",
    "        r\"what's\": \"what is\",\n",
    "        r\"where's\": \"where is\",\n",
    "        r\"when's\": \"when is\",\n",
    "        r\"why's\": \"why is\",\n",
    "        r\"how's\": \"how is\",\n",
    "        r\"would've\": \"would have\",\n",
    "        r\"could've\": \"could have\",\n",
    "        r\"should've\": \"should have\",\n",
    "        r\"might've\": \"might have\",\n",
    "        r\"must've\": \"must have\",\n",
    "        r\"wouldn't\": \"would not\",\n",
    "        r\"couldn't\": \"could not\",\n",
    "        r\"shouldn't\": \"should not\",\n",
    "        r\"mightn't\": \"might not\",\n",
    "        r\"mustn't\": \"must not\",\n",
    "        r\"don't\": \"do not\",\n",
    "        r\"doesn't\": \"does not\",\n",
    "        r\"didn't\": \"did not\",\n",
    "        r\"hasn't\": \"has not\",\n",
    "        r\"haven't\": \"have not\",\n",
    "        r\"hadn't\": \"had not\",\n",
    "        r\"can't've\": \"cannot have\",\n",
    "        r\"shan't've\": \"shall not have\",\n",
    "        r\"wouldn't've\": \"would not have\",\n",
    "        r\"couldn't've\": \"could not have\",\n",
    "        r\"shouldn't've\": \"should not have\",\n",
    "        r\"mightn't've\": \"might not have\",\n",
    "        r\"mustn't've\": \"must not have\",\n",
    "        r\"i'd\": \"i would\",\n",
    "        r\"i'll\": \"i will\",\n",
    "        r\"i've\": \"i have\",\n",
    "        r\"i'm\": \"i am\",\n",
    "        r\"you'd\": \"you would\",\n",
    "        r\"you'll\": \"you will\",\n",
    "        r\"you've\": \"you have\",\n",
    "        r\"you're\": \"you are\",\n",
    "        r\"he'd\": \"he would\",\n",
    "        r\"he'll\": \"he will\",\n",
    "        r\"he's\": \"he is\",\n",
    "        r\"she'd\": \"she would\",\n",
    "        r\"she'll\": \"she will\",\n",
    "        r\"she's\": \"she is\",\n",
    "        r\"it'd\": \"it would\",\n",
    "        r\"it'll\": \"it will\",\n",
    "        r\"it's\": \"it is\",\n",
    "        r\"they'd\": \"they would\",\n",
    "        r\"they'll\": \"they will\",\n",
    "        r\"they're\": \"they are\",\n",
    "        r\"we'd\": \"we would\",\n",
    "        r\"we'll\": \"we will\",\n",
    "        r\"we're\": \"we are\",\n",
    "        r\"there'd\": \"there would\",\n",
    "        r\"that'd\": \"that would\",\n",
    "        r\"who'd\": \"who would\",\n",
    "        r\"who'll\": \"who will\",\n",
    "        r\"who're\": \"who are\",\n",
    "        r\"what've\": \"what have\",\n",
    "        r\"where've\": \"where have\",\n",
    "        r\"when've\": \"when have\",\n",
    "        r\"why've\": \"why have\",\n",
    "        r\"how've\": \"how have\",\n",
    "        r\"lets\": \"let us\",\n",
    "        r\"lets'\": \"let us\",\n",
    "        r\"gonna\": \"going to\",\n",
    "        r\"wanna\": \"want to\",\n",
    "        r\"gotta\": \"got to\",\n",
    "        r\"oughtn't\": \"ought not\",\n",
    "        r\"needn't\": \"need not\",\n",
    "        r\"daren't\": \"dare not\",\n",
    "        r\"maam\": \"madam\",\n",
    "        r\"gimme\": \"give me\",\n",
    "        r\"lemme\": \"let me\",\n",
    "        r\"whatcha\": \"what are you\",\n",
    "        r\"whaddaya\": \"what do you\",\n",
    "        r\"gimme\": \"give me\",\n",
    "        r\"gonna\": \"going to\",\n",
    "        r\"gotta\": \"got to\",\n",
    "        r\"hafta\": \"have to\",\n",
    "        r\"wanna\": \"want to\",\n",
    "        r\"ain't\": \"am not\",\n",
    "        r\"y'all\": \"you all\",\n",
    "        r\"coulda\": \"could have\",\n",
    "        r\"woulda\": \"would have\",\n",
    "        r\"shoulda\": \"should have\",\n",
    "        r\"'bout\": \"about\",\n",
    "        r\"'til\": \"until\",\n",
    "        r\"can't've\": \"cannot have\",\n",
    "        r\"could've\": \"could have\",\n",
    "        r\"might've\": \"might have\",\n",
    "        r\"must've\": \"must have\",\n",
    "        r\"should've\": \"should have\",\n",
    "        r\"would've\": \"would have\",\n",
    "        r\"you'd've\": \"you would have\",\n",
    "    }\n",
    "\n",
    "def contraction_expansion(content):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - content (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "    - str: Text with expanded contractions.\n",
    "    \"\"\"\n",
    "    \n",
    "    for contraction, expanded in contractions.items():\n",
    "        pattern = re.compile(contraction, flags=re.IGNORECASE)\n",
    "        content = pattern.sub(expanded, content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Accents and Diacritics\n",
    "*Sometimes, people use accented characters like é, ö, etc. to signify emphasis on a particular letter during pronunciation. Normalize text by removing accents and diacritical marks from characters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accents(content):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - content (str): The input text to be normalized.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The text with accents and diacritics removed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        normalized = unicodedata.normalize('NFKD', content)\n",
    "        without_accents = ''.join([c for c in normalized if not unicodedata.combining(c)])\n",
    "        return without_accents\n",
    "    except Exception as e:\n",
    "        print(f\"Error in removing accents: {e}\")\n",
    "        return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Extra Whitespace\n",
    "\n",
    "*Normalize text by removing extra spaces and leading/trailing spaces*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_whitespace(text):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - text (str): The input text to be normalized.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The text with extra spaces removed.\n",
    "    \"\"\"\n",
    "    return ' '.join(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminating HTML Tags\n",
    "\n",
    "*In cases where raw text originates from sources such as web scraping or screen capture, it often carries along HTML tags. These tags introduce unwanted noise and contribute little to the comprehension and analysis of the text. Therefore, it becomes necessary to strip them.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - text (str): The input text containing HTML tags.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The text with HTML tags removed.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling URLs\n",
    "\n",
    "*Frequently, individuals include URLs, particularly in social media content, to supplement context with additional information. However, URLs tend to vary across samples and can be considered noise.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def remove_url(text):\n",
    "    return re.sub(r\"(https|http)?:\\S*\", \"\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Standardising and Removing Special Characters\n",
    "\n",
    "*Special characters are non-alphanumeric characters. The characters like %,$,&, etc are special. In most NLP tasks, these characters add no value to text understanding and induce noise into algorithms. We can use regular expressions to remove special characters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_character(content):\n",
    "    content = content.lower()\n",
    "    return re.sub(r'\\W+',' ', content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords\n",
    "\n",
    "*In most cases, stopwords like I, am, me, etc. don’t add any information that can help in modeling. Keeping them in the text introduces unnecessary noise and can significantly increase the dimensionality of feature vectors, which can negatively impact both computation cost and model accuracy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(content):\n",
    "    clean_data = []\n",
    "    for i in content.split():\n",
    "        if i.strip().lower() not in stop_words and i.strip().lower().isalpha():\n",
    "            clean_data.append(i.strip().lower())\n",
    "    return \" \".join(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(content):\n",
    "    # content = correct_typos(content)\n",
    "    mapping_dict = load_mapping_dictionary()\n",
    "    content = remove_extra_whitespace(content)\n",
    "    content = normalize_text(content, mapping_dict)\n",
    "    content = contraction_expansion(content)\n",
    "    content = remove_url(content)\n",
    "    content = remove_accents(content)\n",
    "    # content = remove_html_tags(content)\n",
    "    content = remove_special_character(content)\n",
    "    content = remove_stopwords(content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Data Cleaning to Reviews and Displaying Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 53.3 s\n",
      "Wall time: 3min 14s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Movies</th>\n",
       "      <th>Resenhas</th>\n",
       "      <th>Reviews_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>*Disclaimer: I only watched this movie as a conditional agreement. And I see films for free. I wouldn't be caught dead giving my hard earned money to these idiots.Well, to explain the depth of this 'film', I could write my shortest review, ever. Don't see this movie. It is by far the stupidest, lamest, most lazy, and unbelievably UNFUNNY movie I have ever seen. It is a total disaster. But since my hatred for this movie, and the others like it, extends far beyond one viewing, I think I'll go on for a bit.I don't know any of the people in the movie besides Carmen Electra, Vanessa Minnillo, and Kim Kardashian, but it doesn't matter. They're all horrible, though I think that was the point. The editing is flat out horrible, and possibly blatant continuity errors make this crapfast even crappier than I thought it would be. Now I know that these films are not supposed to be serious at all, but come on, it's film-making 101 that if someone gets a minor facial cut, it should be there in the...</td>\n",
       "      <td>Disaster Movie</td>\n",
       "      <td>* IsenÃ§Ã£o de responsabilidade: eu sÃ³ assisti esse filme como um acordo condicional. E eu vejo filmes de graÃ§a. Eu nÃ£o seria pego morto dando meu dinheiro suado a esses idiotas. Bem, para explicar a profundidade desse 'filme', eu poderia escrever minha crÃ­tica mais curta de todos os tempos. NÃ£o vÃª este filme. Ã de longe o filme mais estÃºpido, lamenta, preguiÃ§oso e inacreditavelmente UNFUNNY que eu jÃ¡ vi. Ã um desastre total. Mas como o meu Ã³dio por este filme e por outros, se estende muito alÃ©m de uma exibiÃ§Ã£o, acho que vou continuar um pouco. NÃ£o conheÃ§o nenhuma das pessoas do filme alÃ©m de Carmen Electra, Vanessa Minnillo, e Kim Kardashian, mas isso nÃ£o importa. Eles sÃ£o todos horrÃ­veis, embora eu ache que esse seja o ponto. A ediÃ§Ã£o Ã© horrÃ­vel e, possivelmente, erros de continuidade flagrantes tornam essa porcaria ainda mais horrÃ­vel do que eu pensava. Agora eu sei que esses filmes nÃ£o devem ser sÃ©rios, mas vamos lÃ¡, Ã© o cinema 101 que se alguÃ©m f...</td>\n",
       "      <td>disclaimer watched conditional agreement see free not caught dead giving hard earned money idiots well explain depth write shortest review ever not see far stupidest lamest lazy unbelievably unfunny ever seen total disaster since hatred others like extends far beyond one viewing think go bit not know people besides carmen electra vanessa minnillo kim kardashian not matter horrible though think point flat horrible possibly blatant continuity errors make crapfast even crappier thought know not supposed serious come making someone gets minor facial cut next shot someone gets cut sword blood least cut though since narnia get away give disaster pass jokes thoughtless mindless physical gags obviously take popular last year late well including best picture nominees know saddest thing stupid not care much money make many cameos sorry ass excuses taking away jobs writers truly deserve attention lionsgate thought better taste ashamed making kind crap jason friedberg aaron seltzer burn hell g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I am writing this in hopes that this gets put over the previous review of this \"film\". How anyone can find this slop entertaining is completely beyond me. First of all a spoof film entitled \"Disaster Movie\", should indeed be a spoof on disaster films. Now I have seen 1 (yes count them, 1) disaster film being spoofed, that being \"Twister\". How does Juno, Iron Man, Batman, The Hulk, Alvin and the Chipmunks, Amy Winehouse, or Hancock register as Disaster films? Selzterwater and Failburg once again have shown that they lack any sort of writing skill and humor. Having unfortunately been tortured with Date Movie and Epic Movie I know exactly what to expect from these two...no plot, no jokes just bad references and cheaply remade scenes from other films. Someone should have informed them that satire is more than just copy and paste from one film to another, though I shouldn't say that because some of these actually just seem to be taken from trailers.There is nothing clever or witty or re...</td>\n",
       "      <td>Disaster Movie</td>\n",
       "      <td>Estou escrevendo isso na esperanÃ§a de que isso seja colocado sobre a revisÃ£o anterior deste \"filme\". Como alguÃ©m pode achar divertido esse desleixo estÃ¡ completamente alÃ©m de mim. Antes de mais nada, um filme de parÃ³dia intitulado \"Filme de desastre\" deveria ser, de fato, uma parÃ³dia de filmes de desastre. Agora eu jÃ¡ vi 1 (sim, conte-os, 1) filme de desastre sendo falsificado, sendo \"Twister\". Como Juno, Homem de Ferro, Batman, O Hulk, Alvin e os Esquilos, Amy Winehouse ou Hancock se registram como filmes de Desastre? Selzterwater e Failburg mostraram mais uma vez que nÃ£o possuem nenhum tipo de habilidade e humor de escrita. Infelizmente, tendo sido torturado com Date Movie e Epic Movie, sei exatamente o que esperar desses dois ... nenhum enredo, nenhuma piada, apenas mÃ¡s referÃªncias e cenas refeitas de outros filmes. AlguÃ©m deveria ter informado a eles que a sÃ¡tira Ã© mais do que apenas copiar e colar de um filme para outro, embora eu nÃ£o deva dizer isso porque algu...</td>\n",
       "      <td>writing hopes gets put previous review anyone find slop entertaining completely beyond first spoof entitled disaster indeed spoof disaster seen yes count disaster spoofed twister juno iron man batman hulk alvin chipmunks amy winehouse hancock register disaster selzterwater failburg shown lack sort writing skill humor unfortunately tortured date epic know exactly expect two jokes bad references cheaply remade someone informed satire copy paste one another though not say actually seem taken trailers nothing clever witty remotely smart way two write cannot believe people still pay see travesties insult audience though enjoy doubt smart enough realize unfortunately not number low enough yes includes negatives rate deserves top worst time right date epic faliure mean meet spartans rather forced hour manos hands fate marathon watch slop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Really, I could write a scathing review of this turd sandwich, but instead, I'm just going to be making a few observations and points I've deduced.There's just no point in watching these movies anymore. Does any reader out there remember Scary Movie? Remember how it was original with a few comedic elements to it? There was slapstick, some funny lines, it was a pretty forgettable comedy, but it was worth the price of admission. Well, That was the last time this premise was funny. STOP MAKING THESE MOVIES. PLEASE.I could call for a boycott of these pieces of monkey sh*t, but we all know there's going to be a line up of pre pubescent annoying little buggers, spouting crappy one liners like, \"THIS IS SPARTA!\" and, \"IM RICK JAMES BITCH\" so these movies will continue to make some form of monetary gain, considering the production value of this movie looks like it cost about 10 cents to make.Don't see this movie. Don't spend any money on it. Go home, rent Airplane, laugh your ass off, and ...</td>\n",
       "      <td>Disaster Movie</td>\n",
       "      <td>Realmente, eu poderia escrever uma crÃ­tica contundente sobre esse sanduÃ­che de cocÃ´, mas, em vez disso, vou fazer algumas observaÃ§Ãµes e pontos que deduzi. NÃ£o hÃ¡ mais sentido assistir a esses filmes. Algum leitor por aÃ­ se lembra do filme de terror? Lembra como era original, com alguns elementos cÃ´micos? Havia palhaÃ§ada, algumas frases engraÃ§adas, era uma comÃ©dia bastante esquecÃ­vel, mas valia o preÃ§o da entrada. Bem, essa foi a Ãºltima vez que essa premissa foi engraÃ§ada. PARE DE FAZER ESTES FILMES. POR FAVOR, eu poderia pedir um boicote a esses pedaÃ§os de macaco, mas todos sabemos que haverÃ¡ uma fila de buggers irritantes e prÃ©-pubescentes, jorrando uns forros ruins como: \"ISTO Ã SPARTA!\" e \"IM RICK JAMES BITCH\", para que esses filmes continuem gerando algum ganho monetÃ¡rio, considerando que o valor de produÃ§Ã£o deste filme parece custar cerca de 10 centavos de dÃ³lar. NÃ£o gaste dinheiro com isso. VÃ¡ para casa, alugue a Airplane, ria e julgue silenciosament...</td>\n",
       "      <td>really write scathing review turd sandwich instead going making observations points deduced point watching anymore reader remember scary remember original comedic elements slapstick funny lines pretty forgettable comedy worth price admission well last time premise funny stop making please call boycott pieces monkey sh know going line pre pubescent annoying little buggers spouting crappy one liners like sparta im rick james bitch continue make form monetary gain considering production value looks like cost cents make not see not spend money go home rent airplane laugh ass silently judge people talking monday favor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>If you saw the other previous spoof movies by these two horrible gentlemen, then you should know that this already will be bad. I'll tell you the truth, if you want to watch it as a brainless person (ironically meant for the stereotypical teenagers, which I am not) then you will laugh at it a bit. But if you judge it, even a little, the movie automatically fails. Why? Never ask that when it comes to these two men.Remember the good old Hollywood days whenever making a movie was about showing people a type of art, and also a story that kept you on the edge of your seat? Well whenever word hit that making films earned you loads of cash, then all these greedy people came in the picture and its quite pathetic. These two are no exception. We still have movie artists (most notably the genius that is Christopher Nolan). But these two guys just...well I've been writing so big words, let me put it in simple terms for these guys...These guys suck, they are not artists, but instead money cravi...</td>\n",
       "      <td>Disaster Movie</td>\n",
       "      <td>Se vocÃª viu os outros filmes falsificados anteriores por esses dois senhores horrÃ­veis, deve saber que isso jÃ¡ serÃ¡ ruim. Vou lhe dizer a verdade, se vocÃª quiser vÃª-lo como uma pessoa sem cÃ©rebro (ironicamente para os adolescentes estereotipados, o que eu nÃ£o sou), entÃ£o vocÃª rirÃ¡ um pouco. Mas se vocÃª julgar, mesmo que um pouco, o filme falha automaticamente. Por quÃª? Nunca pergunte isso quando se trata desses dois homens. Lembre-se dos bons e velhos tempos de Hollywood, sempre que fazer um filme era mostrar Ã s pessoas um tipo de arte e tambÃ©m uma histÃ³ria que o mantinha na ponta do seu assento? Bem, sempre que a notÃ­cia de que fazer filmes ganhava muito dinheiro, entÃ£o todas essas pessoas gananciosas apareciam na imagem e Ã© bastante patÃ©tico. Esses dois nÃ£o sÃ£o exceÃ§Ã£o. Ainda temos artistas de filmes (principalmente o gÃªnio Christopher Nolan). Mas esses dois caras simplesmente ... bem, eu tenho escrito palavras tÃ£o grandes, deixe-me colocar em termos sim...</td>\n",
       "      <td>saw previous spoof two horrible gentlemen know already bad tell truth want watch brainless person ironically meant stereotypical teenagers not laugh bit judge even little automatically fails never ask comes two men remember good old hollywood days whenever making showing people type art also kept edge seat well whenever word hit making earned loads cash greedy people came picture quite pathetic two exception still artists notably genius christopher nolan two guys well writing big words let put simple terms guys guys suck not artists instead money craving whores latest proves even fails easily mind blowing mean nothing funny people usually put best stuff like idiots sometimes knew going bad made bet friends not good idea write paper tell everyone whats good whats bad friends flipped review well warning least not even called nothing artistic original jokes sorry references made throughout pretty much random like hannah montana juno gig actually close spoofing failed referencing inste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>This movie I saw a day early for free and I still feel like I got ripped off. It is totally brain dead. Burping, kicking in the groin and boobs all over the place. Lame. What is wrong with society, that films like this even get made? The parodies were all horrendous, and un-funny. The plot was lackluster at best and the acting was shallow, transparent and really quite unnecessary.Anyone see \"Idiocracy\"? Remember the movie that won all the academy awards in the future? Well this is that movie. I have not seen a more rancid crappy film. \"Date Movie\" was okay, The Scary movies at least had decent plots, but this, this makes \"spoofs\" (if I can be so nice to call it that) for this year 0 for 3, with \"Meet the Spartans\" and \"Superhero Movie\" all falling flat.Well I've wasted even more of my life typing about this sack of cow dung. So all in all, don't see this movie, unless of course your IQ is below 80.Thanks, R</td>\n",
       "      <td>Disaster Movie</td>\n",
       "      <td>Este filme eu vi um dia cedo de graÃ§a e ainda sinto que fui enganado. Ã totalmente morte cerebral. Arrotando, chutando a virilha e os peitos por todo o lugar. Coxo. O que hÃ¡ de errado com a sociedade, que filmes como esse sÃ£o feitos? As parÃ³dias eram todas horrendas e pouco engraÃ§adas. O enredo foi sem brilho, na melhor das hipÃ³teses, e a atuaÃ§Ã£o foi superficial, transparente e realmente bastante desnecessÃ¡ria. AlguÃ©m vÃª \"Idiocracia\"? Lembra do filme que ganhou todos os prÃªmios da academia no futuro? Bem, este Ã© esse filme. Eu nÃ£o vi um filme de baixa qualidade mais ranÃ§oso. \"Date Movie\" foi bom, The Scary Movies pelo menos teve enredos decentes, mas isso faz \"spoofs\" (se Ã© que posso dizer assim) para este ano 0 para 3, com \"Meet the Spartans\" e \"Filme de super-herÃ³is\" todos caindo. Bem, eu perdi ainda mais da minha vida digitando sobre esse saco de esterco de vaca. EntÃ£o, apesar de tudo, nÃ£o assista a este filme, a menos que o seu QI seja inferior a 80.</td>\n",
       "      <td>saw day early free still feel like got ripped totally brain dead burping kicking groin boobs place lame wrong society like even get made parodies horrendous un funny lackluster best shallow transparent really quite unnecessary anyone see idiocracy remember academy awards future well not seen rancid crappy date okay scary least decent plots makes spoofs nice call year meet spartans superhero falling flat well wasted even life typing sack cow dung not see unless course iq thanks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ratings  \\\n",
       "0      1.0   \n",
       "1      1.0   \n",
       "2      1.0   \n",
       "3      1.0   \n",
       "4      1.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Reviews  \\\n",
       "0  *Disclaimer: I only watched this movie as a conditional agreement. And I see films for free. I wouldn't be caught dead giving my hard earned money to these idiots.Well, to explain the depth of this 'film', I could write my shortest review, ever. Don't see this movie. It is by far the stupidest, lamest, most lazy, and unbelievably UNFUNNY movie I have ever seen. It is a total disaster. But since my hatred for this movie, and the others like it, extends far beyond one viewing, I think I'll go on for a bit.I don't know any of the people in the movie besides Carmen Electra, Vanessa Minnillo, and Kim Kardashian, but it doesn't matter. They're all horrible, though I think that was the point. The editing is flat out horrible, and possibly blatant continuity errors make this crapfast even crappier than I thought it would be. Now I know that these films are not supposed to be serious at all, but come on, it's film-making 101 that if someone gets a minor facial cut, it should be there in the...   \n",
       "1  I am writing this in hopes that this gets put over the previous review of this \"film\". How anyone can find this slop entertaining is completely beyond me. First of all a spoof film entitled \"Disaster Movie\", should indeed be a spoof on disaster films. Now I have seen 1 (yes count them, 1) disaster film being spoofed, that being \"Twister\". How does Juno, Iron Man, Batman, The Hulk, Alvin and the Chipmunks, Amy Winehouse, or Hancock register as Disaster films? Selzterwater and Failburg once again have shown that they lack any sort of writing skill and humor. Having unfortunately been tortured with Date Movie and Epic Movie I know exactly what to expect from these two...no plot, no jokes just bad references and cheaply remade scenes from other films. Someone should have informed them that satire is more than just copy and paste from one film to another, though I shouldn't say that because some of these actually just seem to be taken from trailers.There is nothing clever or witty or re...   \n",
       "2  Really, I could write a scathing review of this turd sandwich, but instead, I'm just going to be making a few observations and points I've deduced.There's just no point in watching these movies anymore. Does any reader out there remember Scary Movie? Remember how it was original with a few comedic elements to it? There was slapstick, some funny lines, it was a pretty forgettable comedy, but it was worth the price of admission. Well, That was the last time this premise was funny. STOP MAKING THESE MOVIES. PLEASE.I could call for a boycott of these pieces of monkey sh*t, but we all know there's going to be a line up of pre pubescent annoying little buggers, spouting crappy one liners like, \"THIS IS SPARTA!\" and, \"IM RICK JAMES BITCH\" so these movies will continue to make some form of monetary gain, considering the production value of this movie looks like it cost about 10 cents to make.Don't see this movie. Don't spend any money on it. Go home, rent Airplane, laugh your ass off, and ...   \n",
       "3  If you saw the other previous spoof movies by these two horrible gentlemen, then you should know that this already will be bad. I'll tell you the truth, if you want to watch it as a brainless person (ironically meant for the stereotypical teenagers, which I am not) then you will laugh at it a bit. But if you judge it, even a little, the movie automatically fails. Why? Never ask that when it comes to these two men.Remember the good old Hollywood days whenever making a movie was about showing people a type of art, and also a story that kept you on the edge of your seat? Well whenever word hit that making films earned you loads of cash, then all these greedy people came in the picture and its quite pathetic. These two are no exception. We still have movie artists (most notably the genius that is Christopher Nolan). But these two guys just...well I've been writing so big words, let me put it in simple terms for these guys...These guys suck, they are not artists, but instead money cravi...   \n",
       "4                                                                                 This movie I saw a day early for free and I still feel like I got ripped off. It is totally brain dead. Burping, kicking in the groin and boobs all over the place. Lame. What is wrong with society, that films like this even get made? The parodies were all horrendous, and un-funny. The plot was lackluster at best and the acting was shallow, transparent and really quite unnecessary.Anyone see \"Idiocracy\"? Remember the movie that won all the academy awards in the future? Well this is that movie. I have not seen a more rancid crappy film. \"Date Movie\" was okay, The Scary movies at least had decent plots, but this, this makes \"spoofs\" (if I can be so nice to call it that) for this year 0 for 3, with \"Meet the Spartans\" and \"Superhero Movie\" all falling flat.Well I've wasted even more of my life typing about this sack of cow dung. So all in all, don't see this movie, unless of course your IQ is below 80.Thanks, R   \n",
       "\n",
       "           Movies  \\\n",
       "0  Disaster Movie   \n",
       "1  Disaster Movie   \n",
       "2  Disaster Movie   \n",
       "3  Disaster Movie   \n",
       "4  Disaster Movie   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Resenhas  \\\n",
       "0  * IsenÃ§Ã£o de responsabilidade: eu sÃ³ assisti esse filme como um acordo condicional. E eu vejo filmes de graÃ§a. Eu nÃ£o seria pego morto dando meu dinheiro suado a esses idiotas. Bem, para explicar a profundidade desse 'filme', eu poderia escrever minha crÃ­tica mais curta de todos os tempos. NÃ£o vÃª este filme. Ã de longe o filme mais estÃºpido, lamenta, preguiÃ§oso e inacreditavelmente UNFUNNY que eu jÃ¡ vi. Ã um desastre total. Mas como o meu Ã³dio por este filme e por outros, se estende muito alÃ©m de uma exibiÃ§Ã£o, acho que vou continuar um pouco. NÃ£o conheÃ§o nenhuma das pessoas do filme alÃ©m de Carmen Electra, Vanessa Minnillo, e Kim Kardashian, mas isso nÃ£o importa. Eles sÃ£o todos horrÃ­veis, embora eu ache que esse seja o ponto. A ediÃ§Ã£o Ã© horrÃ­vel e, possivelmente, erros de continuidade flagrantes tornam essa porcaria ainda mais horrÃ­vel do que eu pensava. Agora eu sei que esses filmes nÃ£o devem ser sÃ©rios, mas vamos lÃ¡, Ã© o cinema 101 que se alguÃ©m f...   \n",
       "1  Estou escrevendo isso na esperanÃ§a de que isso seja colocado sobre a revisÃ£o anterior deste \"filme\". Como alguÃ©m pode achar divertido esse desleixo estÃ¡ completamente alÃ©m de mim. Antes de mais nada, um filme de parÃ³dia intitulado \"Filme de desastre\" deveria ser, de fato, uma parÃ³dia de filmes de desastre. Agora eu jÃ¡ vi 1 (sim, conte-os, 1) filme de desastre sendo falsificado, sendo \"Twister\". Como Juno, Homem de Ferro, Batman, O Hulk, Alvin e os Esquilos, Amy Winehouse ou Hancock se registram como filmes de Desastre? Selzterwater e Failburg mostraram mais uma vez que nÃ£o possuem nenhum tipo de habilidade e humor de escrita. Infelizmente, tendo sido torturado com Date Movie e Epic Movie, sei exatamente o que esperar desses dois ... nenhum enredo, nenhuma piada, apenas mÃ¡s referÃªncias e cenas refeitas de outros filmes. AlguÃ©m deveria ter informado a eles que a sÃ¡tira Ã© mais do que apenas copiar e colar de um filme para outro, embora eu nÃ£o deva dizer isso porque algu...   \n",
       "2  Realmente, eu poderia escrever uma crÃ­tica contundente sobre esse sanduÃ­che de cocÃ´, mas, em vez disso, vou fazer algumas observaÃ§Ãµes e pontos que deduzi. NÃ£o hÃ¡ mais sentido assistir a esses filmes. Algum leitor por aÃ­ se lembra do filme de terror? Lembra como era original, com alguns elementos cÃ´micos? Havia palhaÃ§ada, algumas frases engraÃ§adas, era uma comÃ©dia bastante esquecÃ­vel, mas valia o preÃ§o da entrada. Bem, essa foi a Ãºltima vez que essa premissa foi engraÃ§ada. PARE DE FAZER ESTES FILMES. POR FAVOR, eu poderia pedir um boicote a esses pedaÃ§os de macaco, mas todos sabemos que haverÃ¡ uma fila de buggers irritantes e prÃ©-pubescentes, jorrando uns forros ruins como: \"ISTO Ã SPARTA!\" e \"IM RICK JAMES BITCH\", para que esses filmes continuem gerando algum ganho monetÃ¡rio, considerando que o valor de produÃ§Ã£o deste filme parece custar cerca de 10 centavos de dÃ³lar. NÃ£o gaste dinheiro com isso. VÃ¡ para casa, alugue a Airplane, ria e julgue silenciosament...   \n",
       "3  Se vocÃª viu os outros filmes falsificados anteriores por esses dois senhores horrÃ­veis, deve saber que isso jÃ¡ serÃ¡ ruim. Vou lhe dizer a verdade, se vocÃª quiser vÃª-lo como uma pessoa sem cÃ©rebro (ironicamente para os adolescentes estereotipados, o que eu nÃ£o sou), entÃ£o vocÃª rirÃ¡ um pouco. Mas se vocÃª julgar, mesmo que um pouco, o filme falha automaticamente. Por quÃª? Nunca pergunte isso quando se trata desses dois homens. Lembre-se dos bons e velhos tempos de Hollywood, sempre que fazer um filme era mostrar Ã s pessoas um tipo de arte e tambÃ©m uma histÃ³ria que o mantinha na ponta do seu assento? Bem, sempre que a notÃ­cia de que fazer filmes ganhava muito dinheiro, entÃ£o todas essas pessoas gananciosas apareciam na imagem e Ã© bastante patÃ©tico. Esses dois nÃ£o sÃ£o exceÃ§Ã£o. Ainda temos artistas de filmes (principalmente o gÃªnio Christopher Nolan). Mas esses dois caras simplesmente ... bem, eu tenho escrito palavras tÃ£o grandes, deixe-me colocar em termos sim...   \n",
       "4             Este filme eu vi um dia cedo de graÃ§a e ainda sinto que fui enganado. Ã totalmente morte cerebral. Arrotando, chutando a virilha e os peitos por todo o lugar. Coxo. O que hÃ¡ de errado com a sociedade, que filmes como esse sÃ£o feitos? As parÃ³dias eram todas horrendas e pouco engraÃ§adas. O enredo foi sem brilho, na melhor das hipÃ³teses, e a atuaÃ§Ã£o foi superficial, transparente e realmente bastante desnecessÃ¡ria. AlguÃ©m vÃª \"Idiocracia\"? Lembra do filme que ganhou todos os prÃªmios da academia no futuro? Bem, este Ã© esse filme. Eu nÃ£o vi um filme de baixa qualidade mais ranÃ§oso. \"Date Movie\" foi bom, The Scary Movies pelo menos teve enredos decentes, mas isso faz \"spoofs\" (se Ã© que posso dizer assim) para este ano 0 para 3, com \"Meet the Spartans\" e \"Filme de super-herÃ³is\" todos caindo. Bem, eu perdi ainda mais da minha vida digitando sobre esse saco de esterco de vaca. EntÃ£o, apesar de tudo, nÃ£o assista a este filme, a menos que o seu QI seja inferior a 80.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Reviews_clean  \n",
       "0  disclaimer watched conditional agreement see free not caught dead giving hard earned money idiots well explain depth write shortest review ever not see far stupidest lamest lazy unbelievably unfunny ever seen total disaster since hatred others like extends far beyond one viewing think go bit not know people besides carmen electra vanessa minnillo kim kardashian not matter horrible though think point flat horrible possibly blatant continuity errors make crapfast even crappier thought know not supposed serious come making someone gets minor facial cut next shot someone gets cut sword blood least cut though since narnia get away give disaster pass jokes thoughtless mindless physical gags obviously take popular last year late well including best picture nominees know saddest thing stupid not care much money make many cameos sorry ass excuses taking away jobs writers truly deserve attention lionsgate thought better taste ashamed making kind crap jason friedberg aaron seltzer burn hell g...  \n",
       "1                                                                                                                                                               writing hopes gets put previous review anyone find slop entertaining completely beyond first spoof entitled disaster indeed spoof disaster seen yes count disaster spoofed twister juno iron man batman hulk alvin chipmunks amy winehouse hancock register disaster selzterwater failburg shown lack sort writing skill humor unfortunately tortured date epic know exactly expect two jokes bad references cheaply remade someone informed satire copy paste one another though not say actually seem taken trailers nothing clever witty remotely smart way two write cannot believe people still pay see travesties insult audience though enjoy doubt smart enough realize unfortunately not number low enough yes includes negatives rate deserves top worst time right date epic faliure mean meet spartans rather forced hour manos hands fate marathon watch slop  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                             really write scathing review turd sandwich instead going making observations points deduced point watching anymore reader remember scary remember original comedic elements slapstick funny lines pretty forgettable comedy worth price admission well last time premise funny stop making please call boycott pieces monkey sh know going line pre pubescent annoying little buggers spouting crappy one liners like sparta im rick james bitch continue make form monetary gain considering production value looks like cost cents make not see not spend money go home rent airplane laugh ass silently judge people talking monday favor  \n",
       "3  saw previous spoof two horrible gentlemen know already bad tell truth want watch brainless person ironically meant stereotypical teenagers not laugh bit judge even little automatically fails never ask comes two men remember good old hollywood days whenever making showing people type art also kept edge seat well whenever word hit making earned loads cash greedy people came picture quite pathetic two exception still artists notably genius christopher nolan two guys well writing big words let put simple terms guys guys suck not artists instead money craving whores latest proves even fails easily mind blowing mean nothing funny people usually put best stuff like idiots sometimes knew going bad made bet friends not good idea write paper tell everyone whats good whats bad friends flipped review well warning least not even called nothing artistic original jokes sorry references made throughout pretty much random like hannah montana juno gig actually close spoofing failed referencing inste...  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        saw day early free still feel like got ripped totally brain dead burping kicking groin boobs place lame wrong society like even get made parodies horrendous un funny lackluster best shallow transparent really quite unnecessary anyone see idiocracy remember academy awards future well not seen rancid crappy date okay scary least decent plots makes spoofs nice call year meet spartans superhero falling flat well wasted even life typing sack cow dung not see unless course iq thanks  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pd.options.display.max_colwidth = 1000\n",
    "df['Reviews_clean']=df['Reviews'].apply(data_cleaning)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    60000\n",
       "1    60000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'] = df['Ratings'].apply(lambda x: '1' if x >= 7 else ('0' if x<=4 else '2'))\n",
    "\n",
    "df=df[df.Label<'2']\n",
    "data=df[['Reviews_clean','Label']]\n",
    "\n",
    "data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tiled\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tiled\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize  \n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wordnetlemma = WordNetLemmatizer()\n",
    "    \n",
    "    def __call__(self, reviews):\n",
    "        \"\"\"\n",
    "        Lemmatizes a list of tokens to their base form.\n",
    "\n",
    "        Parameters:\n",
    "        - tokens (list): A list of words to be lemmatized.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of lemmatized words.\n",
    "        \"\"\"\n",
    "        return [self.wordnetlemma.lemmatize(word) for word in word_tokenize(reviews)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization with CountVectorizer and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data['Label']\n",
    "y_test = test_data['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(train_data, test_data, tokenizer, ngram_range=(1, 1), min_df=10, max_features=500):\n",
    "    \"\"\"\n",
    "    Vectorizes text data using specified vectorizer type.\n",
    "\n",
    "    Parameters:\n",
    "    - train_data (pd.Series): The training text data to be vectorized.\n",
    "    - test_data (pd.Series): The test text data to be vectorized.\n",
    "    - tokenizer (callable): A tokenizer function or class instance.\n",
    "    - ngram_range (tuple): The range of n-grams to consider.\n",
    "    - min_df (int): Minimum document frequency for terms.\n",
    "    - max_features (int): Maximum number of features to consider.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: The vectorized training text data.\n",
    "    - np.ndarray: The vectorized test text data.\n",
    "    - Vectorizer: The fitted vectorizer instance (TfidfVectorizer).\n",
    "    \"\"\"\n",
    "  \n",
    "    vectorizer = TfidfVectorizer(analyzer=\"word\", \n",
    "                                 tokenizer=tokenizer, \n",
    "                                 ngram_range=ngram_range, \n",
    "                                 min_df=min_df,\n",
    "                                 max_features=max_features)\n",
    "  \n",
    "\n",
    "    x_train_vectorized = vectorizer.fit_transform(train_data).toarray()\n",
    "    x_test_vectorized = vectorizer.transform(test_data).toarray()\n",
    "\n",
    "    return x_train_vectorized, x_test_vectorized, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Logistic Regression with TF-IDF Vectorizers: N-gram Range 1 to 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorization with TfidfVectorizer (N-gram Range: 1 to 3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ngrams_tfidf_vectorized, x_test_ngrams_tfidf_vectorized, ngram_tfidf_vectorizer = vectorize_text(\n",
    "    train_data['Reviews_clean'], \n",
    "    test_data['Reviews_clean'], \n",
    "    tokenizer=LemmaTokenizer(), \n",
    "    ngram_range=(1, 3), \n",
    "    min_df=10, \n",
    "    max_features=5000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 48s\n",
      "Wall time: 23.8 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_1.fit(x_train_ngrams_tfidf_vectorized,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation on Test and Train dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Performance Metrics***\n",
    "\n",
    "1. **Precision Score:**\n",
    "   - **Definition:** Precision is the ratio of correctly predicted positive observations to the total predicted positives. It answers the question: \"Of all the instances that were predicted as positive, how many were actually positive?\"\n",
    "   - **Formula:** \n",
    "     - `Precision = True Positives / (True Positives + False Positives)`\n",
    "   - **Interpretation:** A high precision score indicates a low false positive rate.\n",
    "   \n",
    "\n",
    "2. **AUC Score (Area Under the ROC Curve):**\n",
    "   - **Definition:** AUC measures the ability of the model to distinguish between classes. The ROC curve is a plot of the true positive rate against the false positive rate at various threshold settings.\n",
    "   - **Range:** 0 to 1, where 1 indicates a perfect model and 0.5 suggests no discriminative power.\n",
    "   - **Interpretation:** A higher AUC indicates better model performance.\n",
    "\n",
    "3. **F1 Score:**\n",
    "   - **Definition:** The F1 score is the harmonic mean of precision and recall. It considers both false positives and false negatives.\n",
    "   - **Formula:** \n",
    "     - `F1 Score = 2 * (Precision * Recall) / (Precision + Recall)`\n",
    "   - **Interpretation:** A high F1 score indicates a balance between precision and recall.\n",
    "   - **Note on Recall:** Recall (or Sensitivity) is the ratio of correctly predicted positive observations to all actual positives. It answers the question: \"Of all the instances that were actually positive, how many were predicted as positive?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Training Dataset Metrics***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Training Dataset:**\n",
      "- **Precision Score**: 0.90925\n",
      "- **Recall Score**: 0.90925\n",
      "- **AUC Score**: 0.9681792289615996\n",
      "- **F1 Score**: 0.9092487265586905\n"
     ]
    }
   ],
   "source": [
    "precision_score_training = precision_score(y_train, model_1.predict(x_train_ngrams_tfidf_vectorized), average='micro')\n",
    "recall_score_training = recall_score(y_train, model_1.predict(x_train_ngrams_tfidf_vectorized), average='micro')\n",
    "auc_score_training = roc_auc_score(y_train, model_1.predict_proba(x_train_ngrams_tfidf_vectorized)[:, 1], multi_class='ovo', average='macro')\n",
    "f1_score_training = f1_score(y_train, model_1.predict(x_train_ngrams_tfidf_vectorized), average=\"weighted\")\n",
    "\n",
    "print(\"\\n**Training Dataset:**\")\n",
    "print(f\"- **Precision Score**: {precision_score_training}\")\n",
    "print(f\"- **Recall Score**: {recall_score_training}\")\n",
    "print(f\"- **AUC Score**: {auc_score_training}\")\n",
    "print(f\"- **F1 Score**: {f1_score_training}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Test Dataset Metrics***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Test Dataset:**\n",
      "- **Precision Score**: 0.8913333333333333\n",
      "- **Recall Score**: 0.8913333333333333\n",
      "- **AUC Score**: 0.9575016853183461\n",
      "- **F1 Score**: 0.8913354584211626\n"
     ]
    }
   ],
   "source": [
    "precision_score_test = precision_score(y_test, model_1.predict(x_test_ngrams_tfidf_vectorized), average='micro')\n",
    "recall_score_test = recall_score(y_test, model_1.predict(x_test_ngrams_tfidf_vectorized), average='micro')\n",
    "auc_score_test = roc_auc_score(y_test, model_1.predict_proba(x_test_ngrams_tfidf_vectorized)[:, 1], multi_class='ovo', average='macro')\n",
    "f1_score_test = f1_score(y_test, model_1.predict(x_test_ngrams_tfidf_vectorized), average=\"weighted\")\n",
    "\n",
    "print(\"\\n**Test Dataset:**\")\n",
    "print(f\"- **Precision Score**: {precision_score_test}\")\n",
    "print(f\"- **Recall Score**: {recall_score_test}\")\n",
    "print(f\"- **AUC Score**: {auc_score_test}\")\n",
    "print(f\"- **F1 Score**: {f1_score_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Pipeline(\n",
    "    steps=[\n",
    "    (\"classifier\", DecisionTreeClassifier())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;classifier&#x27;, DecisionTreeClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;classifier&#x27;, DecisionTreeClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('classifier', DecisionTreeClassifier())])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_2.fit(x_train_ngrams_tfidf_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Training Dataset Metrics for Decision Tree Classifier***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Decision Tree Classifier Performance Metrics\n",
      "\n",
      "**Training Dataset:**\n",
      "- **Precision Score**: 0.9999404761904762\n",
      "- **Recall Score**: 0.9999404761904762\n",
      "- **AUC Score**: 0.9999999770404832\n",
      "- **F1 Score**: 0.9999404762037628\n"
     ]
    }
   ],
   "source": [
    "precision_score_training_dt = precision_score(y_train, model_2.predict(x_train_ngrams_tfidf_vectorized), average='micro')\n",
    "recall_score_training_dt = recall_score(y_train, model_2.predict(x_train_ngrams_tfidf_vectorized), average='micro')\n",
    "auc_score_training_dt = roc_auc_score(y_train, model_2.predict_proba(x_train_ngrams_tfidf_vectorized)[:, 1], multi_class='ovo', average='macro')\n",
    "f1_score_training_dt = f1_score(y_train, model_2.predict(x_train_ngrams_tfidf_vectorized), average=\"weighted\")\n",
    "\n",
    "print(\"### Decision Tree Classifier Performance Metrics\")\n",
    "print(\"\\n**Training Dataset:**\")\n",
    "print(f\"- **Precision Score**: {precision_score_training_dt}\")\n",
    "print(f\"- **Recall Score**: {recall_score_training_dt}\")\n",
    "print(f\"- **AUC Score**: {auc_score_training_dt}\")\n",
    "print(f\"- **F1 Score**: {f1_score_training_dt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Test Dataset Metrics for Decision Tree Classifier***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Test Dataset:**\n",
      "- **Precision Score**: 0.7158888888888889\n",
      "- **Recall Score**: 0.7158888888888889\n",
      "- **AUC Score**: 0.7160518300211375\n",
      "- **F1 Score**: 0.7158939083592555\n"
     ]
    }
   ],
   "source": [
    "precision_score_test_dt = precision_score(y_test, model_2.predict(x_test_ngrams_tfidf_vectorized), average='micro')\n",
    "recall_score_test_dt = recall_score(y_test, model_2.predict(x_test_ngrams_tfidf_vectorized), average='micro')\n",
    "auc_score_test_dt = roc_auc_score(y_test, model_2.predict_proba(x_test_ngrams_tfidf_vectorized)[:, 1], multi_class='ovo', average='macro')\n",
    "f1_score_test_dt = f1_score(y_test, model_2.predict(x_test_ngrams_tfidf_vectorized), average=\"weighted\")\n",
    "\n",
    "print(\"\\n**Test Dataset:**\")\n",
    "print(f\"- **Precision Score**: {precision_score_test_dt}\")\n",
    "print(f\"- **Recall Score**: {recall_score_test_dt}\")\n",
    "print(f\"- **AUC Score**: {auc_score_test_dt}\")\n",
    "print(f\"- **F1 Score**: {f1_score_test_dt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observation*\n",
    "\n",
    "The Decision Tree Classifier demonstrates near-perfect performance on the training dataset, with precision, recall, and AUC scores all close to 1, indicating potential overfitting. However, the significant drop in these metrics on the test dataset suggests that the model struggles to generalize well to unseen data, with precision, recall, and AUC scores around 0.716, indicating moderate performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier with max depth 11 to fix overfit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = Pipeline(\n",
    "    steps=[\n",
    "        (\"classifier\", DecisionTreeClassifier(criterion='gini', max_depth=11, min_samples_split=2, min_samples_leaf=1)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 55.7 s\n",
      "Wall time: 2min 10s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;classifier&#x27;, DecisionTreeClassifier(max_depth=15))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;classifier&#x27;, DecisionTreeClassifier(max_depth=15))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=15)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('classifier', DecisionTreeClassifier(max_depth=15))])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_3.fit(x_train_ngrams_tfidf_vectorized,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Decision Tree Classifier with max depth 11 Performance Metrics\n",
      "\n",
      "**Training Dataset:**\n",
      "- **Precision Score**: 0.771452380952381\n",
      "- **Recall Score**: 0.771452380952381\n",
      "- **AUC Score**: 0.8611728585743647\n",
      "- **F1 Score**: 0.768116525928192\n"
     ]
    }
   ],
   "source": [
    "precision_score_training_dt_11 = precision_score(y_train, model_3.predict(x_train_ngrams_tfidf_vectorized), average='micro')\n",
    "recall_score_training_dt_11 = recall_score(y_train, model_3.predict(x_train_ngrams_tfidf_vectorized), average='micro')\n",
    "auc_score_training_dt_11 = roc_auc_score(y_train, model_3.predict_proba(x_train_ngrams_tfidf_vectorized)[:, 1], multi_class='ovo', average='macro')\n",
    "f1_score_training_dt_11 = f1_score(y_train, model_3.predict(x_train_ngrams_tfidf_vectorized), average=\"weighted\")\n",
    "\n",
    "print(\"### Decision Tree Classifier with max depth 11 Performance Metrics\")\n",
    "print(\"\\n**Training Dataset:**\")\n",
    "print(f\"- **Precision Score**: {precision_score_training_dt_11}\")\n",
    "print(f\"- **Recall Score**: {recall_score_training_dt_11}\")\n",
    "print(f\"- **AUC Score**: {auc_score_training_dt_11}\")\n",
    "print(f\"- **F1 Score**: {f1_score_training_dt_11}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Test Dataset:**\n",
      "- **Precision Score**: 0.7209722222222222\n",
      "- **Recall Score**: 0.7209722222222222\n",
      "- **AUC Score**: 0.7797682749007329\n",
      "- **F1 Score**: 0.7174349318364087\n"
     ]
    }
   ],
   "source": [
    "precision_score_test_dt_11 = precision_score(y_test, model_3.predict(x_test_ngrams_tfidf_vectorized), average='micro')\n",
    "recall_score_test_dt_11 = recall_score(y_test, model_3.predict(x_test_ngrams_tfidf_vectorized), average='micro')\n",
    "auc_score_test_dt_11 = roc_auc_score(y_test, model_3.predict_proba(x_test_ngrams_tfidf_vectorized)[:, 1], multi_class='ovo', average='macro')\n",
    "f1_score_test_dt_11 = f1_score(y_test, model_3.predict(x_test_ngrams_tfidf_vectorized), average=\"weighted\")\n",
    "\n",
    "print(\"\\n**Test Dataset:**\")\n",
    "print(f\"- **Precision Score**: {precision_score_test_dt_11}\")\n",
    "print(f\"- **Recall Score**: {recall_score_test_dt_11}\")\n",
    "print(f\"- **AUC Score**: {auc_score_test_dt_11}\")\n",
    "print(f\"- **F1 Score**: {f1_score_test_dt_11}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
