{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: plotly in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (5.24.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: wordcloud in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.9.4)\n",
      "Requirement already satisfied: pyspellchecker in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.8.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: prettytable in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from plotly) (8.2.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\tiled\\appdata\\roaming\\python\\python312\\site-packages (from prettytable) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas seaborn matplotlib plotly scikit-learn nltk wordcloud pyspellchecker beautifulsoup4 prettytable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'C:\\Users\\tiled\\OneDrive\\Desktop\\DataSet\\IMDB-Dataset.csv',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    *Disclaimer: I only watched this movie as a co...\n",
       "1    I am writing this in hopes that this gets put ...\n",
       "2    Really, I could write a scathing review of thi...\n",
       "3    If you saw the other previous spoof movies by ...\n",
       "4    This movie I saw a day early for free and I st...\n",
       "5    Honestly, what is wrong with you, Hollywood? N...\n",
       "6    I was given a free ticket to this film; so I c...\n",
       "7    OK, so \"Disastrous\" isn't an imaginative barb ...\n",
       "8    Jason Friedberg and Aaron Seltzer, the way eve...\n",
       "9    Honestly the worst movie ever made. Theatre fu...\n",
       "Name: Reviews, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)[\"Reviews\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing Stopwords for Enhanced Text Preprocessing\n",
    "\n",
    "*Enhancing the default stopword list by adding domain-specific words and removing negations to improve text analysis accuracy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tiled\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "new_stopwords = [\n",
    "\"would\", \"shall\", \"could\", \"might\", \"movie\", \"movies\", \"film\", \"films\",\n",
    "\"cinema\", \"director\", \"directors\", \"actor\", \"actors\", \"actress\", \"actresses\",\n",
    "\"cast\", \"screenplay\", \"plot\", \"story\", \"character\", \"characters\", \"scene\",\n",
    "\"scenes\", \"soundtrack\", \"soundtracks\",\"sequel\", \"prequel\", \"adaptation\", \"trailer\", \"genre\", \"genres\", \"release\",\n",
    "\"theaters\", \"theatre\", \"performance\", \"performances\", \"acting\", \"blockbuster\",\n",
    "\"indie\", \"oscars\", \"oscar\", \"award-winning\", \"box\", \"office\", \"premiere\",\n",
    "\"rating\", \"ratings\", \"reviews\", \"animation\", \"cinematography\", \"editing\", \"script\", \"narrative\",\n",
    "\"direction\", \"score\", \"soundtrack\"\n",
    "]\n",
    "\n",
    "stop_words.extend(new_stopwords)\n",
    "stop_words.remove(\"not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233\n"
     ]
    }
   ],
   "source": [
    "stop_words=set(stop_words)\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Text Preprocessing Pipeline\n",
    "\n",
    "*Comprehensive functions for cleaning and preparing text data, including removing special characters, URLs, stopwords, and expanding contractions to enhance the quality of the dataset.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction of Typos\n",
    "\n",
    "*Written text often contains errors, such as “Fen” instead of “Fan.” To rectify these errors, a dictionary is employed to map words to their correct forms based on similarity.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "def correct_typos(content):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - content (str): The input text to be corrected.\n",
    "\n",
    "    Returns:\n",
    "    - str: The text with corrected typos.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        words = content.split()\n",
    "        corrected_words = []\n",
    "        misspelled = spell.unknown(words)\n",
    "\n",
    "        for word in words:\n",
    "            if word in misspelled:\n",
    "                corrected_word = spell.correction(word)\n",
    "                corrected_words.append(corrected_word)\n",
    "            else:\n",
    "                corrected_words.append(word)\n",
    "        \n",
    "        corrected_content = ' '.join(corrected_words)\n",
    "        return corrected_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error in correcting typos: {e}\")\n",
    "        return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping and Replacement\n",
    "\n",
    "*This involves mapping words to standardized language equivalents. For instance, words like “b4” and “ttyl,” commonly understood by humans as “before” and “talk to you later,” pose challenges for machines. Normalization entails mapping such words to their standardized counterparts.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def load_mapping_dictionary():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    - dict: A dictionary mapping non-standard words to standardized words.\n",
    "    \"\"\"\n",
    "    mapping_dict = {\n",
    "        \"b4\": \"before\",\n",
    "        \"ttyl\": \"talk to you later\",\n",
    "        \"u\": \"you\",\n",
    "        \"r\": \"are\",\n",
    "        \"lol\": \"laughing out loud\",\n",
    "        \"idk\": \"i do not know\",\n",
    "        \"btw\": \"by the way\",\n",
    "        \"omg\": \"oh my god\",\n",
    "        \"imo\": \"in my opinion\",\n",
    "        \"np\": \"no problem\",\n",
    "        \"gr8\": \"great\",\n",
    "        \"l8r\": \"later\",\n",
    "        \"gtg\": \"got to go\",\n",
    "        \"thx\": \"thanks\",\n",
    "        \"pls\": \"please\",\n",
    "        \"plz\": \"please\",\n",
    "        \"bc\": \"because\",\n",
    "        \"cuz\": \"because\",\n",
    "        \"y'all\": \"you all\",\n",
    "        \"luv\": \"love\",\n",
    "        \"wanna\": \"want to\",\n",
    "        \"gonna\": \"going to\",\n",
    "        \"hafta\": \"have to\",\n",
    "        \"kinda\": \"kind of\",\n",
    "        \"sorta\": \"sort of\",\n",
    "        \"gimme\": \"give me\",\n",
    "        \"lemme\": \"let me\",\n",
    "        \"whatcha\": \"what are you\",\n",
    "        \"whaddaya\": \"what do you\",\n",
    "    }\n",
    "    return mapping_dict\n",
    "\n",
    "def normalize_text(content, mapping_dict):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - content (str): The input text to be normalized.\n",
    "    - mapping_dict (dict): A dictionary mapping non-standard words to standardized words.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The normalized text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        words = content.split()\n",
    "        normalized_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            clean_word = re.sub(r'[^\\w\\s]', '', word.lower())\n",
    "            if clean_word in mapping_dict:\n",
    "                replacement = mapping_dict[clean_word]\n",
    "                normalized_words.append(replacement)\n",
    "            else:\n",
    "                normalized_words.append(word)\n",
    "        \n",
    "        normalized_content = ' '.join(normalized_words)\n",
    "        return normalized_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error in normalizing text: {e}\")\n",
    "        return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding Contractions\n",
    "\n",
    "*Convert contractions like “don’t” to “do not” or “I’ll” to “I will.”*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "        r\"won't\": \"will not\",\n",
    "        r\"can't\": \"cannot\",\n",
    "        r\"i'm\": \"i am\",\n",
    "        r\"ain't\": \"is not\",\n",
    "        r\"let's\": \"let us\",\n",
    "        r\"ma'am\": \"madam\",\n",
    "        r\"shan't\": \"shall not\",\n",
    "        r\"n't\": \" not\",\n",
    "        r\"'re\": \" are\",\n",
    "        r\"'s\": \" is\",\n",
    "        r\"'d\": \" would\",\n",
    "        r\"'ll\": \" will\",\n",
    "        r\"'ve\": \" have\",\n",
    "        r\"'m\": \" am\",\n",
    "        r\"he's\": \"he is\",\n",
    "        r\"she's\": \"she is\",\n",
    "        r\"it's\": \"it is\",\n",
    "        r\"that's\": \"that is\",\n",
    "        r\"there's\": \"there is\",\n",
    "        r\"who's\": \"who is\",\n",
    "        r\"what's\": \"what is\",\n",
    "        r\"where's\": \"where is\",\n",
    "        r\"when's\": \"when is\",\n",
    "        r\"why's\": \"why is\",\n",
    "        r\"how's\": \"how is\",\n",
    "        r\"would've\": \"would have\",\n",
    "        r\"could've\": \"could have\",\n",
    "        r\"should've\": \"should have\",\n",
    "        r\"might've\": \"might have\",\n",
    "        r\"must've\": \"must have\",\n",
    "        r\"wouldn't\": \"would not\",\n",
    "        r\"couldn't\": \"could not\",\n",
    "        r\"shouldn't\": \"should not\",\n",
    "        r\"mightn't\": \"might not\",\n",
    "        r\"mustn't\": \"must not\",\n",
    "        r\"don't\": \"do not\",\n",
    "        r\"doesn't\": \"does not\",\n",
    "        r\"didn't\": \"did not\",\n",
    "        r\"hasn't\": \"has not\",\n",
    "        r\"haven't\": \"have not\",\n",
    "        r\"hadn't\": \"had not\",\n",
    "        r\"can't've\": \"cannot have\",\n",
    "        r\"shan't've\": \"shall not have\",\n",
    "        r\"wouldn't've\": \"would not have\",\n",
    "        r\"couldn't've\": \"could not have\",\n",
    "        r\"shouldn't've\": \"should not have\",\n",
    "        r\"mightn't've\": \"might not have\",\n",
    "        r\"mustn't've\": \"must not have\",\n",
    "        r\"i'd\": \"i would\",\n",
    "        r\"i'll\": \"i will\",\n",
    "        r\"i've\": \"i have\",\n",
    "        r\"i'm\": \"i am\",\n",
    "        r\"you'd\": \"you would\",\n",
    "        r\"you'll\": \"you will\",\n",
    "        r\"you've\": \"you have\",\n",
    "        r\"you're\": \"you are\",\n",
    "        r\"he'd\": \"he would\",\n",
    "        r\"he'll\": \"he will\",\n",
    "        r\"he's\": \"he is\",\n",
    "        r\"she'd\": \"she would\",\n",
    "        r\"she'll\": \"she will\",\n",
    "        r\"she's\": \"she is\",\n",
    "        r\"it'd\": \"it would\",\n",
    "        r\"it'll\": \"it will\",\n",
    "        r\"it's\": \"it is\",\n",
    "        r\"they'd\": \"they would\",\n",
    "        r\"they'll\": \"they will\",\n",
    "        r\"they're\": \"they are\",\n",
    "        r\"we'd\": \"we would\",\n",
    "        r\"we'll\": \"we will\",\n",
    "        r\"we're\": \"we are\",\n",
    "        r\"there'd\": \"there would\",\n",
    "        r\"that'd\": \"that would\",\n",
    "        r\"who'd\": \"who would\",\n",
    "        r\"who'll\": \"who will\",\n",
    "        r\"who're\": \"who are\",\n",
    "        r\"what've\": \"what have\",\n",
    "        r\"where've\": \"where have\",\n",
    "        r\"when've\": \"when have\",\n",
    "        r\"why've\": \"why have\",\n",
    "        r\"how've\": \"how have\",\n",
    "        r\"lets\": \"let us\",\n",
    "        r\"lets'\": \"let us\",\n",
    "        r\"gonna\": \"going to\",\n",
    "        r\"wanna\": \"want to\",\n",
    "        r\"gotta\": \"got to\",\n",
    "        r\"oughtn't\": \"ought not\",\n",
    "        r\"needn't\": \"need not\",\n",
    "        r\"daren't\": \"dare not\",\n",
    "        r\"maam\": \"madam\",\n",
    "        r\"gimme\": \"give me\",\n",
    "        r\"lemme\": \"let me\",\n",
    "        r\"whatcha\": \"what are you\",\n",
    "        r\"whaddaya\": \"what do you\",\n",
    "        r\"gimme\": \"give me\",\n",
    "        r\"gonna\": \"going to\",\n",
    "        r\"gotta\": \"got to\",\n",
    "        r\"hafta\": \"have to\",\n",
    "        r\"wanna\": \"want to\",\n",
    "        r\"ain't\": \"am not\",\n",
    "        r\"y'all\": \"you all\",\n",
    "        r\"coulda\": \"could have\",\n",
    "        r\"woulda\": \"would have\",\n",
    "        r\"shoulda\": \"should have\",\n",
    "        r\"'bout\": \"about\",\n",
    "        r\"'til\": \"until\",\n",
    "        r\"can't've\": \"cannot have\",\n",
    "        r\"could've\": \"could have\",\n",
    "        r\"might've\": \"might have\",\n",
    "        r\"must've\": \"must have\",\n",
    "        r\"should've\": \"should have\",\n",
    "        r\"would've\": \"would have\",\n",
    "        r\"you'd've\": \"you would have\",\n",
    "    }\n",
    "\n",
    "def contraction_expansion(content):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - content (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "    - str: Text with expanded contractions.\n",
    "    \"\"\"\n",
    "    \n",
    "    for contraction, expanded in contractions.items():\n",
    "        pattern = re.compile(contraction, flags=re.IGNORECASE)\n",
    "        content = pattern.sub(expanded, content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Accents and Diacritics\n",
    "*Sometimes, people use accented characters like é, ö, etc. to signify emphasis on a particular letter during pronunciation. Normalize text by removing accents and diacritical marks from characters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accents(content):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - content (str): The input text to be normalized.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The text with accents and diacritics removed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        normalized = unicodedata.normalize('NFKD', content)\n",
    "        without_accents = ''.join([c for c in normalized if not unicodedata.combining(c)])\n",
    "        return without_accents\n",
    "    except Exception as e:\n",
    "        print(f\"Error in removing accents: {e}\")\n",
    "        return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Extra Whitespace\n",
    "\n",
    "*Normalize text by removing extra spaces and leading/trailing spaces*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_whitespace(text):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - text (str): The input text to be normalized.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The text with extra spaces removed.\n",
    "    \"\"\"\n",
    "    return ' '.join(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminating HTML Tags\n",
    "\n",
    "*In cases where raw text originates from sources such as web scraping or screen capture, it often carries along HTML tags. These tags introduce unwanted noise and contribute little to the comprehension and analysis of the text. Therefore, it becomes necessary to strip them.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - text (str): The input text containing HTML tags.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The text with HTML tags removed.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling URLs\n",
    "\n",
    "*Frequently, individuals include URLs, particularly in social media content, to supplement context with additional information. However, URLs tend to vary across samples and can be considered noise.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def remove_url(text):\n",
    "    return re.sub(r\"(https|http)?:\\S*\", \"\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Standardising and Removing Special Characters\n",
    "\n",
    "*Special characters are non-alphanumeric characters. The characters like %,$,&, etc are special. In most NLP tasks, these characters add no value to text understanding and induce noise into algorithms. We can use regular expressions to remove special characters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_character(content):\n",
    "    content = content.lower()\n",
    "    return re.sub(r'\\W+',' ', content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords\n",
    "\n",
    "*In most cases, stopwords like I, am, me, etc. don’t add any information that can help in modeling. Keeping them in the text introduces unnecessary noise and can significantly increase the dimensionality of feature vectors, which can negatively impact both computation cost and model accuracy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(content):\n",
    "    clean_data = []\n",
    "    for i in content.split():\n",
    "        if i.strip().lower() not in stop_words and i.strip().lower().isalpha():\n",
    "            clean_data.append(i.strip().lower())\n",
    "    return \" \".join(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(content):\n",
    "    # content = correct_typos(content)\n",
    "    mapping_dict = load_mapping_dictionary()\n",
    "    content = remove_extra_whitespace(content)\n",
    "    content = normalize_text(content, mapping_dict)\n",
    "    content = contraction_expansion(content)\n",
    "    content = remove_url(content)\n",
    "    content = remove_accents(content)\n",
    "    # content = remove_html_tags(content)\n",
    "    content = remove_special_character(content)\n",
    "    content = remove_stopwords(content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Data Cleaning to Reviews and Displaying Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 57s\n",
      "Wall time: 3min 27s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Movies</th>\n",
       "      <th>Resenhas</th>\n",
       "      <th>Reviews_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>*Disclaimer: I only watched this movie as a conditional agreement. And I see films for free. I wouldn't be caught dead giving my hard earned money to these idiots.Well, to explain the depth of this 'film', I could write my shortest review, ever. Don't see this movie. It is by far the stupidest, lamest, most lazy, and unbelievably UNFUNNY movie I have ever seen. It is a total disaster. But since my hatred for this movie, and the others like it, extends far beyond one viewing, I think I'll go on for a bit.I don't know any of the people in the movie besides Carmen Electra, Vanessa Minnillo, and Kim Kardashian, but it doesn't matter. They're all horrible, though I think that was the point. The editing is flat out horrible, and possibly blatant continuity errors make this crapfast even crappier than I thought it would be. Now I know that these films are not supposed to be serious at all, but come on, it's film-making 101 that if someone gets a minor facial cut, it should be there in the...</td>\n",
       "      <td>Disaster Movie</td>\n",
       "      <td>* IsenÃ§Ã£o de responsabilidade: eu sÃ³ assisti esse filme como um acordo condicional. E eu vejo filmes de graÃ§a. Eu nÃ£o seria pego morto dando meu dinheiro suado a esses idiotas. Bem, para explicar a profundidade desse 'filme', eu poderia escrever minha crÃ­tica mais curta de todos os tempos. NÃ£o vÃª este filme. Ã de longe o filme mais estÃºpido, lamenta, preguiÃ§oso e inacreditavelmente UNFUNNY que eu jÃ¡ vi. Ã um desastre total. Mas como o meu Ã³dio por este filme e por outros, se estende muito alÃ©m de uma exibiÃ§Ã£o, acho que vou continuar um pouco. NÃ£o conheÃ§o nenhuma das pessoas do filme alÃ©m de Carmen Electra, Vanessa Minnillo, e Kim Kardashian, mas isso nÃ£o importa. Eles sÃ£o todos horrÃ­veis, embora eu ache que esse seja o ponto. A ediÃ§Ã£o Ã© horrÃ­vel e, possivelmente, erros de continuidade flagrantes tornam essa porcaria ainda mais horrÃ­vel do que eu pensava. Agora eu sei que esses filmes nÃ£o devem ser sÃ©rios, mas vamos lÃ¡, Ã© o cinema 101 que se alguÃ©m f...</td>\n",
       "      <td>disclaimer watched conditional agreement see free not caught dead giving hard earned money idiots well explain depth write shortest review ever not see far stupidest lamest lazy unbelievably unfunny ever seen total disaster since hatred others like extends far beyond one viewing think go bit not know people besides carmen electra vanessa minnillo kim kardashian not matter horrible though think point flat horrible possibly blatant continuity errors make crapfast even crappier thought know not supposed serious come making someone gets minor facial cut next shot someone gets cut sword blood least cut though since narnia get away give disaster pass jokes thoughtless mindless physical gags obviously take popular last year late well including best picture nominees know saddest thing stupid not care much money make many cameos sorry ass excuses taking away jobs writers truly deserve attention lionsgate thought better taste ashamed making kind crap jason friedberg aaron seltzer burn hell g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I am writing this in hopes that this gets put over the previous review of this \"film\". How anyone can find this slop entertaining is completely beyond me. First of all a spoof film entitled \"Disaster Movie\", should indeed be a spoof on disaster films. Now I have seen 1 (yes count them, 1) disaster film being spoofed, that being \"Twister\". How does Juno, Iron Man, Batman, The Hulk, Alvin and the Chipmunks, Amy Winehouse, or Hancock register as Disaster films? Selzterwater and Failburg once again have shown that they lack any sort of writing skill and humor. Having unfortunately been tortured with Date Movie and Epic Movie I know exactly what to expect from these two...no plot, no jokes just bad references and cheaply remade scenes from other films. Someone should have informed them that satire is more than just copy and paste from one film to another, though I shouldn't say that because some of these actually just seem to be taken from trailers.There is nothing clever or witty or re...</td>\n",
       "      <td>Disaster Movie</td>\n",
       "      <td>Estou escrevendo isso na esperanÃ§a de que isso seja colocado sobre a revisÃ£o anterior deste \"filme\". Como alguÃ©m pode achar divertido esse desleixo estÃ¡ completamente alÃ©m de mim. Antes de mais nada, um filme de parÃ³dia intitulado \"Filme de desastre\" deveria ser, de fato, uma parÃ³dia de filmes de desastre. Agora eu jÃ¡ vi 1 (sim, conte-os, 1) filme de desastre sendo falsificado, sendo \"Twister\". Como Juno, Homem de Ferro, Batman, O Hulk, Alvin e os Esquilos, Amy Winehouse ou Hancock se registram como filmes de Desastre? Selzterwater e Failburg mostraram mais uma vez que nÃ£o possuem nenhum tipo de habilidade e humor de escrita. Infelizmente, tendo sido torturado com Date Movie e Epic Movie, sei exatamente o que esperar desses dois ... nenhum enredo, nenhuma piada, apenas mÃ¡s referÃªncias e cenas refeitas de outros filmes. AlguÃ©m deveria ter informado a eles que a sÃ¡tira Ã© mais do que apenas copiar e colar de um filme para outro, embora eu nÃ£o deva dizer isso porque algu...</td>\n",
       "      <td>writing hopes gets put previous review anyone find slop entertaining completely beyond first spoof entitled disaster indeed spoof disaster seen yes count disaster spoofed twister juno iron man batman hulk alvin chipmunks amy winehouse hancock register disaster selzterwater failburg shown lack sort writing skill humor unfortunately tortured date epic know exactly expect two jokes bad references cheaply remade someone informed satire copy paste one another though not say actually seem taken trailers nothing clever witty remotely smart way two write cannot believe people still pay see travesties insult audience though enjoy doubt smart enough realize unfortunately not number low enough yes includes negatives rate deserves top worst time right date epic faliure mean meet spartans rather forced hour manos hands fate marathon watch slop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Really, I could write a scathing review of this turd sandwich, but instead, I'm just going to be making a few observations and points I've deduced.There's just no point in watching these movies anymore. Does any reader out there remember Scary Movie? Remember how it was original with a few comedic elements to it? There was slapstick, some funny lines, it was a pretty forgettable comedy, but it was worth the price of admission. Well, That was the last time this premise was funny. STOP MAKING THESE MOVIES. PLEASE.I could call for a boycott of these pieces of monkey sh*t, but we all know there's going to be a line up of pre pubescent annoying little buggers, spouting crappy one liners like, \"THIS IS SPARTA!\" and, \"IM RICK JAMES BITCH\" so these movies will continue to make some form of monetary gain, considering the production value of this movie looks like it cost about 10 cents to make.Don't see this movie. Don't spend any money on it. Go home, rent Airplane, laugh your ass off, and ...</td>\n",
       "      <td>Disaster Movie</td>\n",
       "      <td>Realmente, eu poderia escrever uma crÃ­tica contundente sobre esse sanduÃ­che de cocÃ´, mas, em vez disso, vou fazer algumas observaÃ§Ãµes e pontos que deduzi. NÃ£o hÃ¡ mais sentido assistir a esses filmes. Algum leitor por aÃ­ se lembra do filme de terror? Lembra como era original, com alguns elementos cÃ´micos? Havia palhaÃ§ada, algumas frases engraÃ§adas, era uma comÃ©dia bastante esquecÃ­vel, mas valia o preÃ§o da entrada. Bem, essa foi a Ãºltima vez que essa premissa foi engraÃ§ada. PARE DE FAZER ESTES FILMES. POR FAVOR, eu poderia pedir um boicote a esses pedaÃ§os de macaco, mas todos sabemos que haverÃ¡ uma fila de buggers irritantes e prÃ©-pubescentes, jorrando uns forros ruins como: \"ISTO Ã SPARTA!\" e \"IM RICK JAMES BITCH\", para que esses filmes continuem gerando algum ganho monetÃ¡rio, considerando que o valor de produÃ§Ã£o deste filme parece custar cerca de 10 centavos de dÃ³lar. NÃ£o gaste dinheiro com isso. VÃ¡ para casa, alugue a Airplane, ria e julgue silenciosament...</td>\n",
       "      <td>really write scathing review turd sandwich instead going making observations points deduced point watching anymore reader remember scary remember original comedic elements slapstick funny lines pretty forgettable comedy worth price admission well last time premise funny stop making please call boycott pieces monkey sh know going line pre pubescent annoying little buggers spouting crappy one liners like sparta im rick james bitch continue make form monetary gain considering production value looks like cost cents make not see not spend money go home rent airplane laugh ass silently judge people talking monday favor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>If you saw the other previous spoof movies by these two horrible gentlemen, then you should know that this already will be bad. I'll tell you the truth, if you want to watch it as a brainless person (ironically meant for the stereotypical teenagers, which I am not) then you will laugh at it a bit. But if you judge it, even a little, the movie automatically fails. Why? Never ask that when it comes to these two men.Remember the good old Hollywood days whenever making a movie was about showing people a type of art, and also a story that kept you on the edge of your seat? Well whenever word hit that making films earned you loads of cash, then all these greedy people came in the picture and its quite pathetic. These two are no exception. We still have movie artists (most notably the genius that is Christopher Nolan). But these two guys just...well I've been writing so big words, let me put it in simple terms for these guys...These guys suck, they are not artists, but instead money cravi...</td>\n",
       "      <td>Disaster Movie</td>\n",
       "      <td>Se vocÃª viu os outros filmes falsificados anteriores por esses dois senhores horrÃ­veis, deve saber que isso jÃ¡ serÃ¡ ruim. Vou lhe dizer a verdade, se vocÃª quiser vÃª-lo como uma pessoa sem cÃ©rebro (ironicamente para os adolescentes estereotipados, o que eu nÃ£o sou), entÃ£o vocÃª rirÃ¡ um pouco. Mas se vocÃª julgar, mesmo que um pouco, o filme falha automaticamente. Por quÃª? Nunca pergunte isso quando se trata desses dois homens. Lembre-se dos bons e velhos tempos de Hollywood, sempre que fazer um filme era mostrar Ã s pessoas um tipo de arte e tambÃ©m uma histÃ³ria que o mantinha na ponta do seu assento? Bem, sempre que a notÃ­cia de que fazer filmes ganhava muito dinheiro, entÃ£o todas essas pessoas gananciosas apareciam na imagem e Ã© bastante patÃ©tico. Esses dois nÃ£o sÃ£o exceÃ§Ã£o. Ainda temos artistas de filmes (principalmente o gÃªnio Christopher Nolan). Mas esses dois caras simplesmente ... bem, eu tenho escrito palavras tÃ£o grandes, deixe-me colocar em termos sim...</td>\n",
       "      <td>saw previous spoof two horrible gentlemen know already bad tell truth want watch brainless person ironically meant stereotypical teenagers not laugh bit judge even little automatically fails never ask comes two men remember good old hollywood days whenever making showing people type art also kept edge seat well whenever word hit making earned loads cash greedy people came picture quite pathetic two exception still artists notably genius christopher nolan two guys well writing big words let put simple terms guys guys suck not artists instead money craving whores latest proves even fails easily mind blowing mean nothing funny people usually put best stuff like idiots sometimes knew going bad made bet friends not good idea write paper tell everyone whats good whats bad friends flipped review well warning least not even called nothing artistic original jokes sorry references made throughout pretty much random like hannah montana juno gig actually close spoofing failed referencing inste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>This movie I saw a day early for free and I still feel like I got ripped off. It is totally brain dead. Burping, kicking in the groin and boobs all over the place. Lame. What is wrong with society, that films like this even get made? The parodies were all horrendous, and un-funny. The plot was lackluster at best and the acting was shallow, transparent and really quite unnecessary.Anyone see \"Idiocracy\"? Remember the movie that won all the academy awards in the future? Well this is that movie. I have not seen a more rancid crappy film. \"Date Movie\" was okay, The Scary movies at least had decent plots, but this, this makes \"spoofs\" (if I can be so nice to call it that) for this year 0 for 3, with \"Meet the Spartans\" and \"Superhero Movie\" all falling flat.Well I've wasted even more of my life typing about this sack of cow dung. So all in all, don't see this movie, unless of course your IQ is below 80.Thanks, R</td>\n",
       "      <td>Disaster Movie</td>\n",
       "      <td>Este filme eu vi um dia cedo de graÃ§a e ainda sinto que fui enganado. Ã totalmente morte cerebral. Arrotando, chutando a virilha e os peitos por todo o lugar. Coxo. O que hÃ¡ de errado com a sociedade, que filmes como esse sÃ£o feitos? As parÃ³dias eram todas horrendas e pouco engraÃ§adas. O enredo foi sem brilho, na melhor das hipÃ³teses, e a atuaÃ§Ã£o foi superficial, transparente e realmente bastante desnecessÃ¡ria. AlguÃ©m vÃª \"Idiocracia\"? Lembra do filme que ganhou todos os prÃªmios da academia no futuro? Bem, este Ã© esse filme. Eu nÃ£o vi um filme de baixa qualidade mais ranÃ§oso. \"Date Movie\" foi bom, The Scary Movies pelo menos teve enredos decentes, mas isso faz \"spoofs\" (se Ã© que posso dizer assim) para este ano 0 para 3, com \"Meet the Spartans\" e \"Filme de super-herÃ³is\" todos caindo. Bem, eu perdi ainda mais da minha vida digitando sobre esse saco de esterco de vaca. EntÃ£o, apesar de tudo, nÃ£o assista a este filme, a menos que o seu QI seja inferior a 80.</td>\n",
       "      <td>saw day early free still feel like got ripped totally brain dead burping kicking groin boobs place lame wrong society like even get made parodies horrendous un funny lackluster best shallow transparent really quite unnecessary anyone see idiocracy remember academy awards future well not seen rancid crappy date okay scary least decent plots makes spoofs nice call year meet spartans superhero falling flat well wasted even life typing sack cow dung not see unless course iq thanks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ratings  \\\n",
       "0      1.0   \n",
       "1      1.0   \n",
       "2      1.0   \n",
       "3      1.0   \n",
       "4      1.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Reviews  \\\n",
       "0  *Disclaimer: I only watched this movie as a conditional agreement. And I see films for free. I wouldn't be caught dead giving my hard earned money to these idiots.Well, to explain the depth of this 'film', I could write my shortest review, ever. Don't see this movie. It is by far the stupidest, lamest, most lazy, and unbelievably UNFUNNY movie I have ever seen. It is a total disaster. But since my hatred for this movie, and the others like it, extends far beyond one viewing, I think I'll go on for a bit.I don't know any of the people in the movie besides Carmen Electra, Vanessa Minnillo, and Kim Kardashian, but it doesn't matter. They're all horrible, though I think that was the point. The editing is flat out horrible, and possibly blatant continuity errors make this crapfast even crappier than I thought it would be. Now I know that these films are not supposed to be serious at all, but come on, it's film-making 101 that if someone gets a minor facial cut, it should be there in the...   \n",
       "1  I am writing this in hopes that this gets put over the previous review of this \"film\". How anyone can find this slop entertaining is completely beyond me. First of all a spoof film entitled \"Disaster Movie\", should indeed be a spoof on disaster films. Now I have seen 1 (yes count them, 1) disaster film being spoofed, that being \"Twister\". How does Juno, Iron Man, Batman, The Hulk, Alvin and the Chipmunks, Amy Winehouse, or Hancock register as Disaster films? Selzterwater and Failburg once again have shown that they lack any sort of writing skill and humor. Having unfortunately been tortured with Date Movie and Epic Movie I know exactly what to expect from these two...no plot, no jokes just bad references and cheaply remade scenes from other films. Someone should have informed them that satire is more than just copy and paste from one film to another, though I shouldn't say that because some of these actually just seem to be taken from trailers.There is nothing clever or witty or re...   \n",
       "2  Really, I could write a scathing review of this turd sandwich, but instead, I'm just going to be making a few observations and points I've deduced.There's just no point in watching these movies anymore. Does any reader out there remember Scary Movie? Remember how it was original with a few comedic elements to it? There was slapstick, some funny lines, it was a pretty forgettable comedy, but it was worth the price of admission. Well, That was the last time this premise was funny. STOP MAKING THESE MOVIES. PLEASE.I could call for a boycott of these pieces of monkey sh*t, but we all know there's going to be a line up of pre pubescent annoying little buggers, spouting crappy one liners like, \"THIS IS SPARTA!\" and, \"IM RICK JAMES BITCH\" so these movies will continue to make some form of monetary gain, considering the production value of this movie looks like it cost about 10 cents to make.Don't see this movie. Don't spend any money on it. Go home, rent Airplane, laugh your ass off, and ...   \n",
       "3  If you saw the other previous spoof movies by these two horrible gentlemen, then you should know that this already will be bad. I'll tell you the truth, if you want to watch it as a brainless person (ironically meant for the stereotypical teenagers, which I am not) then you will laugh at it a bit. But if you judge it, even a little, the movie automatically fails. Why? Never ask that when it comes to these two men.Remember the good old Hollywood days whenever making a movie was about showing people a type of art, and also a story that kept you on the edge of your seat? Well whenever word hit that making films earned you loads of cash, then all these greedy people came in the picture and its quite pathetic. These two are no exception. We still have movie artists (most notably the genius that is Christopher Nolan). But these two guys just...well I've been writing so big words, let me put it in simple terms for these guys...These guys suck, they are not artists, but instead money cravi...   \n",
       "4                                                                                 This movie I saw a day early for free and I still feel like I got ripped off. It is totally brain dead. Burping, kicking in the groin and boobs all over the place. Lame. What is wrong with society, that films like this even get made? The parodies were all horrendous, and un-funny. The plot was lackluster at best and the acting was shallow, transparent and really quite unnecessary.Anyone see \"Idiocracy\"? Remember the movie that won all the academy awards in the future? Well this is that movie. I have not seen a more rancid crappy film. \"Date Movie\" was okay, The Scary movies at least had decent plots, but this, this makes \"spoofs\" (if I can be so nice to call it that) for this year 0 for 3, with \"Meet the Spartans\" and \"Superhero Movie\" all falling flat.Well I've wasted even more of my life typing about this sack of cow dung. So all in all, don't see this movie, unless of course your IQ is below 80.Thanks, R   \n",
       "\n",
       "           Movies  \\\n",
       "0  Disaster Movie   \n",
       "1  Disaster Movie   \n",
       "2  Disaster Movie   \n",
       "3  Disaster Movie   \n",
       "4  Disaster Movie   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Resenhas  \\\n",
       "0  * IsenÃ§Ã£o de responsabilidade: eu sÃ³ assisti esse filme como um acordo condicional. E eu vejo filmes de graÃ§a. Eu nÃ£o seria pego morto dando meu dinheiro suado a esses idiotas. Bem, para explicar a profundidade desse 'filme', eu poderia escrever minha crÃ­tica mais curta de todos os tempos. NÃ£o vÃª este filme. Ã de longe o filme mais estÃºpido, lamenta, preguiÃ§oso e inacreditavelmente UNFUNNY que eu jÃ¡ vi. Ã um desastre total. Mas como o meu Ã³dio por este filme e por outros, se estende muito alÃ©m de uma exibiÃ§Ã£o, acho que vou continuar um pouco. NÃ£o conheÃ§o nenhuma das pessoas do filme alÃ©m de Carmen Electra, Vanessa Minnillo, e Kim Kardashian, mas isso nÃ£o importa. Eles sÃ£o todos horrÃ­veis, embora eu ache que esse seja o ponto. A ediÃ§Ã£o Ã© horrÃ­vel e, possivelmente, erros de continuidade flagrantes tornam essa porcaria ainda mais horrÃ­vel do que eu pensava. Agora eu sei que esses filmes nÃ£o devem ser sÃ©rios, mas vamos lÃ¡, Ã© o cinema 101 que se alguÃ©m f...   \n",
       "1  Estou escrevendo isso na esperanÃ§a de que isso seja colocado sobre a revisÃ£o anterior deste \"filme\". Como alguÃ©m pode achar divertido esse desleixo estÃ¡ completamente alÃ©m de mim. Antes de mais nada, um filme de parÃ³dia intitulado \"Filme de desastre\" deveria ser, de fato, uma parÃ³dia de filmes de desastre. Agora eu jÃ¡ vi 1 (sim, conte-os, 1) filme de desastre sendo falsificado, sendo \"Twister\". Como Juno, Homem de Ferro, Batman, O Hulk, Alvin e os Esquilos, Amy Winehouse ou Hancock se registram como filmes de Desastre? Selzterwater e Failburg mostraram mais uma vez que nÃ£o possuem nenhum tipo de habilidade e humor de escrita. Infelizmente, tendo sido torturado com Date Movie e Epic Movie, sei exatamente o que esperar desses dois ... nenhum enredo, nenhuma piada, apenas mÃ¡s referÃªncias e cenas refeitas de outros filmes. AlguÃ©m deveria ter informado a eles que a sÃ¡tira Ã© mais do que apenas copiar e colar de um filme para outro, embora eu nÃ£o deva dizer isso porque algu...   \n",
       "2  Realmente, eu poderia escrever uma crÃ­tica contundente sobre esse sanduÃ­che de cocÃ´, mas, em vez disso, vou fazer algumas observaÃ§Ãµes e pontos que deduzi. NÃ£o hÃ¡ mais sentido assistir a esses filmes. Algum leitor por aÃ­ se lembra do filme de terror? Lembra como era original, com alguns elementos cÃ´micos? Havia palhaÃ§ada, algumas frases engraÃ§adas, era uma comÃ©dia bastante esquecÃ­vel, mas valia o preÃ§o da entrada. Bem, essa foi a Ãºltima vez que essa premissa foi engraÃ§ada. PARE DE FAZER ESTES FILMES. POR FAVOR, eu poderia pedir um boicote a esses pedaÃ§os de macaco, mas todos sabemos que haverÃ¡ uma fila de buggers irritantes e prÃ©-pubescentes, jorrando uns forros ruins como: \"ISTO Ã SPARTA!\" e \"IM RICK JAMES BITCH\", para que esses filmes continuem gerando algum ganho monetÃ¡rio, considerando que o valor de produÃ§Ã£o deste filme parece custar cerca de 10 centavos de dÃ³lar. NÃ£o gaste dinheiro com isso. VÃ¡ para casa, alugue a Airplane, ria e julgue silenciosament...   \n",
       "3  Se vocÃª viu os outros filmes falsificados anteriores por esses dois senhores horrÃ­veis, deve saber que isso jÃ¡ serÃ¡ ruim. Vou lhe dizer a verdade, se vocÃª quiser vÃª-lo como uma pessoa sem cÃ©rebro (ironicamente para os adolescentes estereotipados, o que eu nÃ£o sou), entÃ£o vocÃª rirÃ¡ um pouco. Mas se vocÃª julgar, mesmo que um pouco, o filme falha automaticamente. Por quÃª? Nunca pergunte isso quando se trata desses dois homens. Lembre-se dos bons e velhos tempos de Hollywood, sempre que fazer um filme era mostrar Ã s pessoas um tipo de arte e tambÃ©m uma histÃ³ria que o mantinha na ponta do seu assento? Bem, sempre que a notÃ­cia de que fazer filmes ganhava muito dinheiro, entÃ£o todas essas pessoas gananciosas apareciam na imagem e Ã© bastante patÃ©tico. Esses dois nÃ£o sÃ£o exceÃ§Ã£o. Ainda temos artistas de filmes (principalmente o gÃªnio Christopher Nolan). Mas esses dois caras simplesmente ... bem, eu tenho escrito palavras tÃ£o grandes, deixe-me colocar em termos sim...   \n",
       "4             Este filme eu vi um dia cedo de graÃ§a e ainda sinto que fui enganado. Ã totalmente morte cerebral. Arrotando, chutando a virilha e os peitos por todo o lugar. Coxo. O que hÃ¡ de errado com a sociedade, que filmes como esse sÃ£o feitos? As parÃ³dias eram todas horrendas e pouco engraÃ§adas. O enredo foi sem brilho, na melhor das hipÃ³teses, e a atuaÃ§Ã£o foi superficial, transparente e realmente bastante desnecessÃ¡ria. AlguÃ©m vÃª \"Idiocracia\"? Lembra do filme que ganhou todos os prÃªmios da academia no futuro? Bem, este Ã© esse filme. Eu nÃ£o vi um filme de baixa qualidade mais ranÃ§oso. \"Date Movie\" foi bom, The Scary Movies pelo menos teve enredos decentes, mas isso faz \"spoofs\" (se Ã© que posso dizer assim) para este ano 0 para 3, com \"Meet the Spartans\" e \"Filme de super-herÃ³is\" todos caindo. Bem, eu perdi ainda mais da minha vida digitando sobre esse saco de esterco de vaca. EntÃ£o, apesar de tudo, nÃ£o assista a este filme, a menos que o seu QI seja inferior a 80.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Reviews_clean  \n",
       "0  disclaimer watched conditional agreement see free not caught dead giving hard earned money idiots well explain depth write shortest review ever not see far stupidest lamest lazy unbelievably unfunny ever seen total disaster since hatred others like extends far beyond one viewing think go bit not know people besides carmen electra vanessa minnillo kim kardashian not matter horrible though think point flat horrible possibly blatant continuity errors make crapfast even crappier thought know not supposed serious come making someone gets minor facial cut next shot someone gets cut sword blood least cut though since narnia get away give disaster pass jokes thoughtless mindless physical gags obviously take popular last year late well including best picture nominees know saddest thing stupid not care much money make many cameos sorry ass excuses taking away jobs writers truly deserve attention lionsgate thought better taste ashamed making kind crap jason friedberg aaron seltzer burn hell g...  \n",
       "1                                                                                                                                                               writing hopes gets put previous review anyone find slop entertaining completely beyond first spoof entitled disaster indeed spoof disaster seen yes count disaster spoofed twister juno iron man batman hulk alvin chipmunks amy winehouse hancock register disaster selzterwater failburg shown lack sort writing skill humor unfortunately tortured date epic know exactly expect two jokes bad references cheaply remade someone informed satire copy paste one another though not say actually seem taken trailers nothing clever witty remotely smart way two write cannot believe people still pay see travesties insult audience though enjoy doubt smart enough realize unfortunately not number low enough yes includes negatives rate deserves top worst time right date epic faliure mean meet spartans rather forced hour manos hands fate marathon watch slop  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                             really write scathing review turd sandwich instead going making observations points deduced point watching anymore reader remember scary remember original comedic elements slapstick funny lines pretty forgettable comedy worth price admission well last time premise funny stop making please call boycott pieces monkey sh know going line pre pubescent annoying little buggers spouting crappy one liners like sparta im rick james bitch continue make form monetary gain considering production value looks like cost cents make not see not spend money go home rent airplane laugh ass silently judge people talking monday favor  \n",
       "3  saw previous spoof two horrible gentlemen know already bad tell truth want watch brainless person ironically meant stereotypical teenagers not laugh bit judge even little automatically fails never ask comes two men remember good old hollywood days whenever making showing people type art also kept edge seat well whenever word hit making earned loads cash greedy people came picture quite pathetic two exception still artists notably genius christopher nolan two guys well writing big words let put simple terms guys guys suck not artists instead money craving whores latest proves even fails easily mind blowing mean nothing funny people usually put best stuff like idiots sometimes knew going bad made bet friends not good idea write paper tell everyone whats good whats bad friends flipped review well warning least not even called nothing artistic original jokes sorry references made throughout pretty much random like hannah montana juno gig actually close spoofing failed referencing inste...  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        saw day early free still feel like got ripped totally brain dead burping kicking groin boobs place lame wrong society like even get made parodies horrendous un funny lackluster best shallow transparent really quite unnecessary anyone see idiocracy remember academy awards future well not seen rancid crappy date okay scary least decent plots makes spoofs nice call year meet spartans superhero falling flat well wasted even life typing sack cow dung not see unless course iq thanks  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pd.options.display.max_colwidth = 1000\n",
    "df['Reviews_clean']=df['Reviews'].apply(data_cleaning)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    60000\n",
       "1    60000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'] = df['Ratings'].apply(lambda x: '1' if x >= 7 else ('0' if x<=4 else '2'))\n",
    "\n",
    "df=df[df.Label<'2']\n",
    "data=df[['Reviews_clean','Reviews','Ratings','Label']]\n",
    "\n",
    "data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tiled\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tiled\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize  \n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wordnetlemma = WordNetLemmatizer()\n",
    "    \n",
    "    def __call__(self, reviews):\n",
    "        \"\"\"\n",
    "        Lemmatizes a list of tokens to their base form.\n",
    "\n",
    "        Parameters:\n",
    "        - tokens (list): A list of words to be lemmatized.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of lemmatized words.\n",
    "        \"\"\"\n",
    "        return [self.wordnetlemma.lemmatize(word) for word in word_tokenize(reviews)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization with CountVectorizer and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data['Label']\n",
    "y_test = test_data['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Reviews_clean', 'Reviews', 'Ratings', 'Label'], dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_list=y_test.tolist()\n",
    "test_list=test_data['Reviews_clean'].tolist()\n",
    "rating_list=test_data['Ratings'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(train_data, test_data, tokenizer, ngram_range=(1, 1), min_df=10, max_features=500):\n",
    "    \"\"\"\n",
    "    Vectorizes text data using specified vectorizer type.\n",
    "\n",
    "    Parameters:\n",
    "    - train_data (pd.Series): The training text data to be vectorized.\n",
    "    - test_data (pd.Series): The test text data to be vectorized.\n",
    "    - tokenizer (callable): A tokenizer function or class instance.\n",
    "    - ngram_range (tuple): The range of n-grams to consider.\n",
    "    - min_df (int): Minimum document frequency for terms.\n",
    "    - max_features (int): Maximum number of features to consider.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: The vectorized training text data.\n",
    "    - np.ndarray: The vectorized test text data.\n",
    "    - Vectorizer: The fitted vectorizer instance (TfidfVectorizer).\n",
    "    \"\"\"\n",
    "  \n",
    "    vectorizer = TfidfVectorizer(analyzer=\"word\", \n",
    "                                 tokenizer=tokenizer, \n",
    "                                 ngram_range=ngram_range, \n",
    "                                 min_df=min_df,\n",
    "                                 max_features=max_features)\n",
    "  \n",
    "\n",
    "    x_train_vectorized = vectorizer.fit_transform(train_data).toarray()\n",
    "    x_test_vectorized = vectorizer.transform(test_data).toarray()\n",
    "\n",
    "    return x_train_vectorized, x_test_vectorized, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tiled\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "x_train_ngrams_tfidf_vectorized, x_test_ngrams_tfidf_vectorized, ngram_tfidf_vectorizer = vectorize_text(\n",
    "    train_data['Reviews_clean'], \n",
    "    test_data['Reviews_clean'], \n",
    "    tokenizer=LemmaTokenizer(), \n",
    "    ngram_range=(1, 3), \n",
    "    min_df=10, \n",
    "    max_features=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define a Logistic Regression model with specific hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_logistic_regression = LogisticRegression(\n",
    "    penalty='l2',\n",
    "    dual=False,\n",
    "    tol=0.0001,\n",
    "    C=10,\n",
    "    solver='lbfgs',\n",
    "    max_iter=200,\n",
    "    multi_class='auto',\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    "    n_jobs=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define a pipeline with TF-IDF vectorization and Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_logistic_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('vect', TfidfVectorizer(\n",
    "            analyzer=\"word\",\n",
    "            tokenizer=LemmaTokenizer(),\n",
    "            ngram_range=(1, 3),\n",
    "            min_df=10,\n",
    "            max_features=10000\n",
    "        )),\n",
    "        ('classifier', LogisticRegression(\n",
    "            penalty='l2',\n",
    "            dual=False,\n",
    "            tol=0.0001,\n",
    "            C=10,\n",
    "            solver='lbfgs',\n",
    "            max_iter=100,\n",
    "            multi_class='auto',\n",
    "            verbose=0,\n",
    "            warm_start=False,\n",
    "            n_jobs=None\n",
    "        ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 12min 46s\n",
      "Wall time: 2min 18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tiled\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=10, max_iter=200)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=10, max_iter=200)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=10, max_iter=200)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "basic_logistic_regression.fit(x_train_ngrams_tfidf_vectorized,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tiled\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 2s\n",
      "Wall time: 2min 39s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tiled\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                 TfidfVectorizer(max_features=10000, min_df=10,\n",
       "                                 ngram_range=(1, 3),\n",
       "                                 tokenizer=&lt;__main__.LemmaTokenizer object at 0x0000026D2BEC9640&gt;)),\n",
       "                (&#x27;classifier&#x27;, LogisticRegression(C=10))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                 TfidfVectorizer(max_features=10000, min_df=10,\n",
       "                                 ngram_range=(1, 3),\n",
       "                                 tokenizer=&lt;__main__.LemmaTokenizer object at 0x0000026D2BEC9640&gt;)),\n",
       "                (&#x27;classifier&#x27;, LogisticRegression(C=10))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=10000, min_df=10, ngram_range=(1, 3),\n",
       "                tokenizer=&lt;__main__.LemmaTokenizer object at 0x0000026D2BEC9640&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=10)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 TfidfVectorizer(max_features=10000, min_df=10,\n",
       "                                 ngram_range=(1, 3),\n",
       "                                 tokenizer=<__main__.LemmaTokenizer object at 0x0000026D2BEC9640>)),\n",
       "                ('classifier', LogisticRegression(C=10))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tfidf_logistic_pipeline.fit(train_data['Reviews_clean'],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score, accuracy_score , confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the basic Logistic Regression model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score for Logistic Regression: 0.89\n",
      "Recall Score for Logistic Regression: 0.89\n",
      "AUC Score for Logistic Regression: 0.96\n",
      "F1 Score for Logistic Regression: 0.89\n",
      "Accuracy Score for Logistic Regression: 0.89\n"
     ]
    }
   ],
   "source": [
    "precision_logistic = precision_score(y_test, basic_logistic_regression.predict(x_test_ngrams_tfidf_vectorized), average='micro')\n",
    "recall_logistic = recall_score(y_test, basic_logistic_regression.predict(x_test_ngrams_tfidf_vectorized), average='micro')\n",
    "auc_logistic = roc_auc_score(y_test, basic_logistic_regression.predict_proba(x_test_ngrams_tfidf_vectorized)[:, 1], multi_class='ovo', average='macro')\n",
    "f1_logistic = f1_score(y_test, basic_logistic_regression.predict(x_test_ngrams_tfidf_vectorized), average=\"weighted\")\n",
    "accuracy_logistic = accuracy_score(y_test, basic_logistic_regression.predict(x_test_ngrams_tfidf_vectorized))\n",
    "\n",
    "print(\"Precision Score for Logistic Regression: {:.2f}\".format(precision_logistic))\n",
    "print(\"Recall Score for Logistic Regression: {:.2f}\".format(recall_logistic))\n",
    "print(\"AUC Score for Logistic Regression: {:.2f}\".format(auc_logistic))\n",
    "print(\"F1 Score for Logistic Regression: {:.2f}\".format(f1_logistic))\n",
    "print(\"Accuracy Score for Logistic Regression: {:.2f}\".format(accuracy_logistic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the TF-IDF Logistic Regression Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score for Logistic Regression Pipeline: 0.89\n",
      "Recall Score for Logistic Regression Pipeline: 0.89\n",
      "AUC Score for Logistic Regression Pipeline: 0.96\n",
      "F1 Score for Logistic Regression Pipeline: 0.89\n",
      "Accuracy Score for Logistic Regression Pipeline: 0.89\n"
     ]
    }
   ],
   "source": [
    "precision_pipeline = precision_score(y_test, tfidf_logistic_pipeline.predict(test_data['Reviews_clean']), average='micro')\n",
    "recall_pipeline = recall_score(y_test, tfidf_logistic_pipeline.predict(test_data['Reviews_clean']), average='micro')\n",
    "auc_pipeline = roc_auc_score(y_test, tfidf_logistic_pipeline.predict_proba(test_data['Reviews_clean'])[:, 1], multi_class='ovo', average='macro')\n",
    "f1_pipeline = f1_score(y_test, tfidf_logistic_pipeline.predict(test_data['Reviews_clean']), average=\"weighted\")\n",
    "accuracy_pipeline = accuracy_score(y_test, tfidf_logistic_pipeline.predict(test_data['Reviews_clean']))\n",
    "\n",
    "print(\"Precision Score for Logistic Regression Pipeline: {:.2f}\".format(precision_pipeline))\n",
    "print(\"Recall Score for Logistic Regression Pipeline: {:.2f}\".format(recall_pipeline))\n",
    "print(\"AUC Score for Logistic Regression Pipeline: {:.2f}\".format(auc_pipeline))\n",
    "print(\"F1 Score for Logistic Regression Pipeline: {:.2f}\".format(f1_pipeline))\n",
    "print(\"Accuracy Score for Logistic Regression Pipeline: {:.2f}\".format(accuracy_pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Interpret and print the results***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_sentiment(output):\n",
    "    return \"Positive\" if output == \"1\" else \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sentiment(review_text, real_sentiment, model):\n",
    "    \"\"\"\n",
    "    Evaluates the sentiment of a given review text using the provided model and compares it to the real sentiment.\n",
    "\n",
    "    Parameters:\n",
    "    - review_text (str): The text of the review to evaluate.\n",
    "    - real_sentiment (str): The actual sentiment of the review (\"Positive\" or \"Negative\").\n",
    "    - model: The trained sentiment analysis model.\n",
    "\n",
    "    Returns:\n",
    "    - None: Prints the real sentiment, predicted sentiment, and whether the prediction was correct.\n",
    "    \"\"\"\n",
    "   \n",
    "    output = model.predict([review_text])\n",
    "\n",
    "    predicted_sentiment = interpret_sentiment(output[0])\n",
    "\n",
    "    print(f\"Real Sentiment: {real_sentiment}\")\n",
    "    print(f\"Predicted Sentiment: {predicted_sentiment}\")\n",
    "\n",
    "    if real_sentiment == predicted_sentiment:\n",
    "        print(\"The model predicted the sentiment correctly.\")\n",
    "    else:\n",
    "        print(\"The model did not predict the sentiment correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model with IMDB Movie Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Sentiment: Positive\n",
      "Predicted Sentiment: Positive\n",
      "The model predicted the sentiment correctly.\n"
     ]
    }
   ],
   "source": [
    "review_text = (\n",
    "    \"Beautiful is not a word that I enjoy using for a film with such a muted color pallet. \"\n",
    "    \"However, for Robert Eggers' Nosferatu, after viewing it in its entirety, it is the only word that comes to mind. \"\n",
    "    \"Eggers has constructed this film meticulously, with a precision seen in very few directors these days. \"\n",
    "    \"Heck, more so than any previous effort he's put to screen. You can tell he enjoys working with a bigger budget \"\n",
    "    \"and he knows what to do with it as well. Lavish, gothic set design and costumes across the board. \"\n",
    "    \"Wonderful special FX, both CG and practical makeup. The cinematography was insane. Sweeping camera shots and \"\n",
    "    \"uncomfortable closeups, all framed in a 35mm letterbox format. It was a spectacle worth the wait, especially on \"\n",
    "    \"the IMAX screen I saw it on. The acting was phenomenal. Lily Rose-Depp should win awards for her all-out performance. \"\n",
    "    \"Both horrific and touching all at the same time. Willem Dafoe provided some much-needed spice for the second half. \"\n",
    "    \"Without his character, the film would've slipped a star as the story starts getting a little long-winded towards the end. \"\n",
    "    \"My only gripe with the cast was Bill Skarsgård as Count Orlok. I felt he could've played into the character's sensitivity \"\n",
    "    \"a bit more. He was cold-hearted, yes, but he did still have love in his heart. The story, adapted from both Dracula and \"\n",
    "    \"Nosferatu... but mostly Nosferatu, was very well written. With dialog that felt natural and authentic to the time period, \"\n",
    "    \"another one of Eggers' specialties, the film flowed rather nicely and almost transports you to that time and place. \"\n",
    "    \"Impressively, when the film does eventually start getting bogged down a bit, Eggers makes sure to pick things right back \"\n",
    "    \"up again to never keep the audience bored, and with a film like this that could've happened easily. Overall, a pretty great \"\n",
    "    \"adaptation of a classic story. Equal parts horror and mystery, and a bittersweet ending that almost made me cry. I loved it, \"\n",
    "    \"and a nice addition to Robert Eggers' growing filmography.\"\n",
    ")\n",
    "\n",
    "real_sentiment = \"Positive\"\n",
    "\n",
    "evaluate_sentiment(review_text, real_sentiment, tfidf_logistic_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Sentiment: Negative\n",
      "Predicted Sentiment: Negative\n",
      "The model predicted the sentiment correctly.\n"
     ]
    }
   ],
   "source": [
    "new_review_text = (\n",
    "    \"Poor content, poor backdrops, scenes without a meaning, predictable outcomes and a plot that is a recycled loop. \"\n",
    "    \"If it's gonna be one of the first series you will ever see it might look good. Otherwise naaaaah!\"\n",
    ")\n",
    "\n",
    "real_sentiment = \"Negative\"\n",
    "\n",
    "evaluate_sentiment(review_text, real_sentiment, tfidf_logistic_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Sentiment: Negative\n",
      "Predicted Sentiment: Negative\n",
      "The model predicted the sentiment correctly.\n"
     ]
    }
   ],
   "source": [
    "review_text = (\n",
    "    \"Story is good. Not a fan of one of the characters getting engaged with an 8 year old girl who he marries at 11 after she gets her period. \"\n",
    "    \"Very disturbing and pulls focus away attention from the rest of the story that is interesting on its own without the pedophilia. \"\n",
    "    \"While understand it was a different time, it clearly was frowned upon and very creepy. I'm curious why no one questioned why he would be interested in a little girl instead of a grown woman. \"\n",
    "    \"I'm episode 6 and waiting for the supernatural aspect to kick in which at this point isn't much. Hopefully the last episodes redeem the show but I doubt it.\"\n",
    ")\n",
    "\n",
    "real_sentiment = \"Negative\"\n",
    "\n",
    "evaluate_sentiment(review_text, real_sentiment, tfidf_logistic_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Sentiment: Positive\n",
      "Predicted Sentiment: Negative\n",
      "The model did not predict the sentiment correctly.\n"
     ]
    }
   ],
   "source": [
    "review_text = (\n",
    "    \"Adapting One Hundred Years of Solitude into a cinematic work appears to be an extremely difficult task for two main reasons. \"\n",
    "    \"First, this novel belongs to a specific literary movement, pioneered by South American writers, particularly Gabriel García Márquez, \"\n",
    "    \"where the story is narrated through the blending of reality and fantasy. At times, this fusion is so intense that separating the two seems impossible. \"\n",
    "    \"Naturally, translating such scenes into cinema risks making them appear absurd and failing to achieve a satisfactory cinematic form. \"\n",
    "    \"However, watching the first episode of the series One Hundred Years of Solitude revealed that the creators managed to convey this magical and surreal feeling \"\n",
    "    \"to the audience without making it seem ridiculous. \"\n",
    "    \"The second reason is the inherent complexity of the novel. One Hundred Years of Solitude is challenging to read due to its repeated use of identical names for different characters, \"\n",
    "    \"as well as its non-linear narrative and frequent disruptions of the timeline. These elements may exhaust the reader. \"\n",
    "    \"Fortunately, such issues are absent in the series, which has successfully transformed the non-linear narrative into a linear one, allowing it to establish a strong connection with the audience.\"\n",
    ")\n",
    "\n",
    "real_sentiment = \"Positive\"\n",
    "\n",
    "evaluate_sentiment(review_text, real_sentiment, tfidf_logistic_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Directions for Improving Sentiment Analysis\n",
    "\n",
    "*Issue and Cause*\n",
    "\n",
    "- **Issue**: The model struggles to accurately predict sentiment for short reviews, despite achieving high accuracy (89%) on longer reviews from the IMDB dataset.\n",
    "\n",
    "- **Cause**:\n",
    "  - **Training Data Bias**: The model is trained primarily on longer reviews, which provide more context and complex patterns for sentiment analysis.\n",
    "  - **Polarity and Sarcasm**: Words like \"like\" and \"hate\" may appear in both positive and negative contexts, especially in longer reviews, making it difficult for the model to determine sentiment in short reviews.\n",
    "  - **Negation Handling**: Short reviews may lack sufficient context for the model to correctly interpret negations or sarcasm.\n",
    "\n",
    "## Solutions\n",
    "\n",
    "1. **Augment Training Data**:\n",
    "   - Add more short reviews to the training dataset, ensuring a balanced representation of positive and negative sentiments. This helps the model learn to recognize sentiment in brief statements.\n",
    "\n",
    "2. **Use Pre-trained Embeddings**:\n",
    "   - Incorporate pre-trained word embeddings like GloVe or Word2Vec to provide richer semantic information for individual words, aiding the model in understanding sentiment in short reviews.\n",
    "\n",
    "3. **Fine-tune the Model**:\n",
    "   - Fine-tune the model on a dataset that includes a variety of review lengths, including short ones, to improve its adaptability to different contexts.\n",
    "\n",
    "4. **Feature Engineering**:\n",
    "   - Add features that capture sentiment more directly, such as sentiment lexicons or part-of-speech tags, to help the model better understand short reviews.\n",
    "\n",
    "5. **Model Complexity**:\n",
    "   - Experiment with more complex models, such as neural networks, which might capture nuances in short reviews better than simpler models.\n",
    "\n",
    "6. **Hyperparameter Tuning**:\n",
    "   - Adjust hyperparameters to improve the model's ability to handle short reviews, potentially increasing its sensitivity to context and sentiment cues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: 'I like this movie.' - Sentiment: Negative\n",
      "Review: 'I hate this movie.' - Sentiment: Positive\n",
      "Review: 'This movie is great!' - Sentiment: Positive\n",
      "Review: 'This movie is terrible.' - Sentiment: Negative\n",
      "Review: 'Amazing film!' - Sentiment: Positive\n",
      "Review: 'Awful film.' - Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "short_reviews = [\n",
    "    \"I like this movie.\",\n",
    "    \"I hate this movie.\",\n",
    "    \"This movie is great!\",\n",
    "    \"This movie is terrible.\",\n",
    "    \"Amazing film!\",\n",
    "    \"Awful film.\"\n",
    "]\n",
    "\n",
    "for review in short_reviews:\n",
    "    output = tfidf_logistic_pipeline.predict([review])\n",
    "    print(f\"Review: '{review}' - Sentiment: {interpret_sentiment(output[0])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
