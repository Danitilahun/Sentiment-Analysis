{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\tiled\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\tiled\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: seaborn in c:\\users\\tiled\\anaconda3\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\tiled\\anaconda3\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: plotly in c:\\users\\tiled\\anaconda3\\lib\\site-packages (5.9.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tiled\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\tiled\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.4-cp311-cp311-win_amd64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tiled\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tiled\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\tiled\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\tiled\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tiled\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tiled\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\tiled\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tiled\\anaconda3\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\tiled\\anaconda3\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\tiled\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\tiled\\anaconda3\\lib\\site-packages (from plotly) (8.2.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\tiled\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\tiled\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tiled\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\tiled\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\tiled\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tiled\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tiled\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tiled\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading wordcloud-1.9.4-cp311-cp311-win_amd64.whl (299 kB)\n",
      "   ---------------------------------------- 0.0/299.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/299.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/299.9 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 20.5/299.9 kB 108.9 kB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 30.7/299.9 kB 163.8 kB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 30.7/299.9 kB 163.8 kB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 41.0/299.9 kB 151.3 kB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 41.0/299.9 kB 151.3 kB/s eta 0:00:02\n",
      "   ------------ --------------------------- 92.2/299.9 kB 261.7 kB/s eta 0:00:01\n",
      "   ------------ --------------------------- 92.2/299.9 kB 261.7 kB/s eta 0:00:01\n",
      "   ------------- -------------------------- 102.4/299.9 kB 227.0 kB/s eta 0:00:01\n",
      "   ------------- -------------------------- 102.4/299.9 kB 227.0 kB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 122.9/299.9 kB 218.3 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 174.1/299.9 kB 299.5 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 174.1/299.9 kB 299.5 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 174.1/299.9 kB 299.5 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 174.1/299.9 kB 299.5 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 174.1/299.9 kB 299.5 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 174.1/299.9 kB 299.5 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 174.1/299.9 kB 299.5 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 174.1/299.9 kB 299.5 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 174.1/299.9 kB 299.5 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 174.1/299.9 kB 299.5 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 174.1/299.9 kB 299.5 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 174.1/299.9 kB 299.5 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 174.1/299.9 kB 299.5 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 174.1/299.9 kB 299.5 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 174.1/299.9 kB 299.5 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 174.1/299.9 kB 299.5 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 174.1/299.9 kB 299.5 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 174.1/299.9 kB 299.5 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 225.3/299.9 kB 156.4 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 225.3/299.9 kB 156.4 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 225.3/299.9 kB 156.4 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 225.3/299.9 kB 156.4 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 225.3/299.9 kB 156.4 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 225.3/299.9 kB 156.4 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 225.3/299.9 kB 156.4 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 225.3/299.9 kB 156.4 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 225.3/299.9 kB 156.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 286.7/299.9 kB 152.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 286.7/299.9 kB 152.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 286.7/299.9 kB 152.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 286.7/299.9 kB 152.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 299.9/299.9 kB 144.8 kB/s eta 0:00:00\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas seaborn matplotlib plotly scikit-learn nltk wordcloud pyspellchecker beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'C:\\Users\\tiled\\OneDrive\\Desktop\\DataSet\\IMDB-Dataset.csv',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    *Disclaimer: I only watched this movie as a co...\n",
       "1    I am writing this in hopes that this gets put ...\n",
       "2    Really, I could write a scathing review of thi...\n",
       "3    If you saw the other previous spoof movies by ...\n",
       "4    This movie I saw a day early for free and I st...\n",
       "5    Honestly, what is wrong with you, Hollywood? N...\n",
       "6    I was given a free ticket to this film; so I c...\n",
       "7    OK, so \"Disastrous\" isn't an imaginative barb ...\n",
       "8    Jason Friedberg and Aaron Seltzer, the way eve...\n",
       "9    Honestly the worst movie ever made. Theatre fu...\n",
       "Name: Reviews, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)[\"Reviews\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing Stopwords for Enhanced Text Preprocessing\n",
    "\n",
    "*Enhancing the default stopword list by adding domain-specific words and removing negations to improve text analysis accuracy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "new_stopwords = [\n",
    "\"would\", \"shall\", \"could\", \"might\", \"movie\", \"movies\", \"film\", \"films\",\n",
    "\"cinema\", \"director\", \"directors\", \"actor\", \"actors\", \"actress\", \"actresses\",\n",
    "\"cast\", \"screenplay\", \"plot\", \"story\", \"character\", \"characters\", \"scene\",\n",
    "\"scenes\", \"soundtrack\", \"soundtracks\",\"sequel\", \"prequel\", \"adaptation\", \"trailer\", \"genre\", \"genres\", \"release\",\n",
    "\"theaters\", \"theatre\", \"performance\", \"performances\", \"acting\", \"blockbuster\",\n",
    "\"indie\", \"oscars\", \"oscar\", \"award-winning\", \"box\", \"office\", \"premiere\",\n",
    "\"rating\", \"ratings\", \"reviews\", \"animation\", \"cinematography\", \"editing\", \"script\", \"narrative\",\n",
    "\"direction\", \"score\", \"soundtrack\"\n",
    "]\n",
    "\n",
    "stop_words.extend(new_stopwords)\n",
    "stop_words.remove(\"not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233\n"
     ]
    }
   ],
   "source": [
    "stop_words=set(stop_words)\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Text Preprocessing Pipeline\n",
    "\n",
    "*Comprehensive functions for cleaning and preparing text data, including removing special characters, URLs, stopwords, and expanding contractions to enhance the quality of the dataset.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction of Typos\n",
    "\n",
    "*Written text often contains errors, such as “Fen” instead of “Fan.” To rectify these errors, a dictionary is employed to map words to their correct forms based on similarity.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "def correct_typos(content):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - content (str): The input text to be corrected.\n",
    "\n",
    "    Returns:\n",
    "    - str: The text with corrected typos.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        words = content.split()\n",
    "        corrected_words = []\n",
    "        misspelled = spell.unknown(words)\n",
    "\n",
    "        for word in words:\n",
    "            if word in misspelled:\n",
    "                corrected_word = spell.correction(word)\n",
    "                corrected_words.append(corrected_word)\n",
    "            else:\n",
    "                corrected_words.append(word)\n",
    "        \n",
    "        corrected_content = ' '.join(corrected_words)\n",
    "        return corrected_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error in correcting typos: {e}\")\n",
    "        return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping and Replacement\n",
    "\n",
    "*This involves mapping words to standardized language equivalents. For instance, words like “b4” and “ttyl,” commonly understood by humans as “before” and “talk to you later,” pose challenges for machines. Normalization entails mapping such words to their standardized counterparts.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def load_mapping_dictionary():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    - dict: A dictionary mapping non-standard words to standardized words.\n",
    "    \"\"\"\n",
    "    mapping_dict = {\n",
    "        \"b4\": \"before\",\n",
    "        \"ttyl\": \"talk to you later\",\n",
    "        \"u\": \"you\",\n",
    "        \"r\": \"are\",\n",
    "        \"lol\": \"laughing out loud\",\n",
    "        \"idk\": \"i do not know\",\n",
    "        \"btw\": \"by the way\",\n",
    "        \"omg\": \"oh my god\",\n",
    "        \"imo\": \"in my opinion\",\n",
    "        \"np\": \"no problem\",\n",
    "        \"gr8\": \"great\",\n",
    "        \"l8r\": \"later\",\n",
    "        \"gtg\": \"got to go\",\n",
    "        \"thx\": \"thanks\",\n",
    "        \"pls\": \"please\",\n",
    "        \"plz\": \"please\",\n",
    "        \"bc\": \"because\",\n",
    "        \"cuz\": \"because\",\n",
    "        \"y'all\": \"you all\",\n",
    "        \"luv\": \"love\",\n",
    "        \"wanna\": \"want to\",\n",
    "        \"gonna\": \"going to\",\n",
    "        \"hafta\": \"have to\",\n",
    "        \"kinda\": \"kind of\",\n",
    "        \"sorta\": \"sort of\",\n",
    "        \"gimme\": \"give me\",\n",
    "        \"lemme\": \"let me\",\n",
    "        \"whatcha\": \"what are you\",\n",
    "        \"whaddaya\": \"what do you\",\n",
    "    }\n",
    "    return mapping_dict\n",
    "\n",
    "def normalize_text(content, mapping_dict):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - content (str): The input text to be normalized.\n",
    "    - mapping_dict (dict): A dictionary mapping non-standard words to standardized words.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The normalized text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        words = content.split()\n",
    "        normalized_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            clean_word = re.sub(r'[^\\w\\s]', '', word.lower())\n",
    "            if clean_word in mapping_dict:\n",
    "                replacement = mapping_dict[clean_word]\n",
    "                normalized_words.append(replacement)\n",
    "            else:\n",
    "                normalized_words.append(word)\n",
    "        \n",
    "        normalized_content = ' '.join(normalized_words)\n",
    "        return normalized_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error in normalizing text: {e}\")\n",
    "        return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding Contractions\n",
    "\n",
    "*Convert contractions like “don’t” to “do not” or “I’ll” to “I will.”*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contraction_expansion(content):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - content (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "    - str: Text with expanded contractions.\n",
    "    \"\"\"\n",
    "    contractions = {\n",
    "        r\"won't\": \"will not\",\n",
    "        r\"can't\": \"cannot\",\n",
    "        r\"i'm\": \"i am\",\n",
    "        r\"ain't\": \"is not\",\n",
    "        r\"let's\": \"let us\",\n",
    "        r\"ma'am\": \"madam\",\n",
    "        r\"shan't\": \"shall not\",\n",
    "        r\"n't\": \" not\",\n",
    "        r\"'re\": \" are\",\n",
    "        r\"'s\": \" is\",\n",
    "        r\"'d\": \" would\",\n",
    "        r\"'ll\": \" will\",\n",
    "        r\"'ve\": \" have\",\n",
    "        r\"'m\": \" am\",\n",
    "        r\"he's\": \"he is\",\n",
    "        r\"she's\": \"she is\",\n",
    "        r\"it's\": \"it is\",\n",
    "        r\"that's\": \"that is\",\n",
    "        r\"there's\": \"there is\",\n",
    "        r\"who's\": \"who is\",\n",
    "        r\"what's\": \"what is\",\n",
    "        r\"where's\": \"where is\",\n",
    "        r\"when's\": \"when is\",\n",
    "        r\"why's\": \"why is\",\n",
    "        r\"how's\": \"how is\",\n",
    "        r\"would've\": \"would have\",\n",
    "        r\"could've\": \"could have\",\n",
    "        r\"should've\": \"should have\",\n",
    "        r\"might've\": \"might have\",\n",
    "        r\"must've\": \"must have\",\n",
    "        r\"wouldn't\": \"would not\",\n",
    "        r\"couldn't\": \"could not\",\n",
    "        r\"shouldn't\": \"should not\",\n",
    "        r\"mightn't\": \"might not\",\n",
    "        r\"mustn't\": \"must not\",\n",
    "        r\"don't\": \"do not\",\n",
    "        r\"doesn't\": \"does not\",\n",
    "        r\"didn't\": \"did not\",\n",
    "        r\"hasn't\": \"has not\",\n",
    "        r\"haven't\": \"have not\",\n",
    "        r\"hadn't\": \"had not\",\n",
    "        r\"can't've\": \"cannot have\",\n",
    "        r\"shan't've\": \"shall not have\",\n",
    "        r\"wouldn't've\": \"would not have\",\n",
    "        r\"couldn't've\": \"could not have\",\n",
    "        r\"shouldn't've\": \"should not have\",\n",
    "        r\"mightn't've\": \"might not have\",\n",
    "        r\"mustn't've\": \"must not have\",\n",
    "        r\"i'd\": \"i would\",\n",
    "        r\"i'll\": \"i will\",\n",
    "        r\"i've\": \"i have\",\n",
    "        r\"i'm\": \"i am\",\n",
    "        r\"you'd\": \"you would\",\n",
    "        r\"you'll\": \"you will\",\n",
    "        r\"you've\": \"you have\",\n",
    "        r\"you're\": \"you are\",\n",
    "        r\"he'd\": \"he would\",\n",
    "        r\"he'll\": \"he will\",\n",
    "        r\"he's\": \"he is\",\n",
    "        r\"she'd\": \"she would\",\n",
    "        r\"she'll\": \"she will\",\n",
    "        r\"she's\": \"she is\",\n",
    "        r\"it'd\": \"it would\",\n",
    "        r\"it'll\": \"it will\",\n",
    "        r\"it's\": \"it is\",\n",
    "        r\"they'd\": \"they would\",\n",
    "        r\"they'll\": \"they will\",\n",
    "        r\"they're\": \"they are\",\n",
    "        r\"we'd\": \"we would\",\n",
    "        r\"we'll\": \"we will\",\n",
    "        r\"we're\": \"we are\",\n",
    "        r\"there'd\": \"there would\",\n",
    "        r\"that'd\": \"that would\",\n",
    "        r\"who'd\": \"who would\",\n",
    "        r\"who'll\": \"who will\",\n",
    "        r\"who're\": \"who are\",\n",
    "        r\"what've\": \"what have\",\n",
    "        r\"where've\": \"where have\",\n",
    "        r\"when've\": \"when have\",\n",
    "        r\"why've\": \"why have\",\n",
    "        r\"how've\": \"how have\",\n",
    "        r\"lets\": \"let us\",\n",
    "        r\"lets'\": \"let us\",\n",
    "        r\"gonna\": \"going to\",\n",
    "        r\"wanna\": \"want to\",\n",
    "        r\"gotta\": \"got to\",\n",
    "        r\"oughtn't\": \"ought not\",\n",
    "        r\"needn't\": \"need not\",\n",
    "        r\"daren't\": \"dare not\",\n",
    "        r\"maam\": \"madam\",\n",
    "        r\"gimme\": \"give me\",\n",
    "        r\"lemme\": \"let me\",\n",
    "        r\"whatcha\": \"what are you\",\n",
    "        r\"whaddaya\": \"what do you\",\n",
    "        r\"gimme\": \"give me\",\n",
    "        r\"gonna\": \"going to\",\n",
    "        r\"gotta\": \"got to\",\n",
    "        r\"hafta\": \"have to\",\n",
    "        r\"wanna\": \"want to\",\n",
    "        r\"ain't\": \"am not\",\n",
    "        r\"y'all\": \"you all\",\n",
    "        r\"coulda\": \"could have\",\n",
    "        r\"woulda\": \"would have\",\n",
    "        r\"shoulda\": \"should have\",\n",
    "        r\"'bout\": \"about\",\n",
    "        r\"'til\": \"until\",\n",
    "        r\"can't've\": \"cannot have\",\n",
    "        r\"could've\": \"could have\",\n",
    "        r\"might've\": \"might have\",\n",
    "        r\"must've\": \"must have\",\n",
    "        r\"should've\": \"should have\",\n",
    "        r\"would've\": \"would have\",\n",
    "        r\"you'd've\": \"you would have\",\n",
    "    }\n",
    "\n",
    "    for contraction, expanded in contractions.items():\n",
    "        pattern = re.compile(contraction, flags=re.IGNORECASE)\n",
    "        content = pattern.sub(expanded, content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Accents and Diacritics\n",
    "*Sometimes, people use accented characters like é, ö, etc. to signify emphasis on a particular letter during pronunciation. Normalize text by removing accents and diacritical marks from characters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accents(content):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - content (str): The input text to be normalized.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The text with accents and diacritics removed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        normalized = unicodedata.normalize('NFKD', content)\n",
    "        without_accents = ''.join([c for c in normalized if not unicodedata.combining(c)])\n",
    "        return without_accents\n",
    "    except Exception as e:\n",
    "        print(f\"Error in removing accents: {e}\")\n",
    "        return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Extra Whitespace\n",
    "\n",
    "*Normalize text by removing extra spaces and leading/trailing spaces*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_whitespace(text):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - text (str): The input text to be normalized.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The text with extra spaces removed.\n",
    "    \"\"\"\n",
    "    return ' '.join(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminating HTML Tags\n",
    "\n",
    "*In cases where raw text originates from sources such as web scraping or screen capture, it often carries along HTML tags. These tags introduce unwanted noise and contribute little to the comprehension and analysis of the text. Therefore, it becomes necessary to strip them.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - text (str): The input text containing HTML tags.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The text with HTML tags removed.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling URLs\n",
    "\n",
    "*Frequently, individuals include URLs, particularly in social media content, to supplement context with additional information. However, URLs tend to vary across samples and can be considered noise.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def remove_url(text):\n",
    "    return re.sub(r\"(https|http)?:\\S*\", \"\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Standardising and Removing Special Characters\n",
    "\n",
    "*Special characters are non-alphanumeric characters. The characters like %,$,&, etc are special. In most NLP tasks, these characters add no value to text understanding and induce noise into algorithms. We can use regular expressions to remove special characters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_character(content):\n",
    "    content = content.lower()\n",
    "    return re.sub(r'\\W+',' ', content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords\n",
    "\n",
    "*In most cases, stopwords like I, am, me, etc. don’t add any information that can help in modeling. Keeping them in the text introduces unnecessary noise and can significantly increase the dimensionality of feature vectors, which can negatively impact both computation cost and model accuracy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(content):\n",
    "    clean_data = []\n",
    "    for i in content.split():\n",
    "        if i.strip().lower() not in stop_words and i.strip().lower().isalpha():\n",
    "            clean_data.append(i.strip().lower())\n",
    "    return \" \".join(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(content):\n",
    "    \n",
    "    content = correct_typos(content)\n",
    "    mapping_dict = load_mapping_dictionary()\n",
    "    content = normalize_text(content, mapping_dict)\n",
    "    content = remove_extra_whitespace(content)\n",
    "    content = remove_url(content)\n",
    "    content = remove_html_tags(content)\n",
    "    content = remove_special_character(content)\n",
    "    content = contraction_expansion(content)\n",
    "    content = remove_accents(content)\n",
    "    content = remove_stopwords(content)\n",
    "    return content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
